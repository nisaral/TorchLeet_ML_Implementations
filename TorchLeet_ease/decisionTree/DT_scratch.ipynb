{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04e4e09",
   "metadata": {},
   "source": [
    "Problem 1: Decision Tree for Classification (with Gini Index)\n",
    "Problem Statement\n",
    "You are tasked with implementing a decision tree classifier from scratch using NumPy, using the Gini index as the splitting criterion. The decision tree will classify synthetic 2D data points into two classes based on feature thresholds. The implementation will include tree construction, prediction, and evaluation using accuracy.\n",
    "Mathematical Definition:\n",
    "\n",
    "Gini Index for a node:\n",
    "$$\\text{Gini} = 1 - \\sum_{i=1}^c p_i^2$$\n",
    "where $ p_i $ is the proportion of class $ i $ in the node, and $ c $ is the number of classes.\n",
    "Split Criterion: Choose the feature and threshold that minimize the weighted Gini index of child nodes:\n",
    "$$\\text{Gini}_{\\text{split}} = \\frac{n_{\\text{left}}}{n} \\text{Gini}_{\\text{left}} + \\frac{n_{\\text{right}}}{n} \\text{Gini}_{\\text{right}}$$\n",
    "\n",
    "Prediction: Assign the majority class of the leaf node.\n",
    "\n",
    "Requirements\n",
    "\n",
    "Implement a DecisionTreeClassifier class with methods for:\n",
    "\n",
    "fit: Build the tree using recursive splitting.\n",
    "predict: Classify new data points.\n",
    "\n",
    "\n",
    "Use the Gini index to select splits.\n",
    "Handle binary classification with 2D synthetic data.\n",
    "Evaluate accuracy on a test set.\n",
    "\n",
    "Constraints\n",
    "\n",
    "Use only NumPy for data manipulation and tree logic.\n",
    "No scikit-learn or other ML libraries.\n",
    "Max tree depth of 3 to prevent overfitting.\n",
    "Handle batch inputs for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(100, 2) * 10\n",
    "\n",
    "y = (X[:, 0] + X[:, 1] > 10).astype(int)\n",
    "\n",
    "# Define the Decision Tree Classifier\n",
    "class DecisionTreeClassifier:\n",
    "    # Purpose: Define a decision tree classifier for binary classification.\n",
    "    # Theory: Decision trees recursively split the feature space into regions based on feature thresholds, using Gini index to optimize splits.\n",
    "    \n",
    "    def __init__(self, max_depth=3):\n",
    "        # Purpose: Initialize the decision tree with a maximum depth.\n",
    "        # Theory: max_depth limits tree growth to prevent overfitting. A depth of 3 balances complexity and generalization for this small dataset.\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        # Purpose: Store the maximum depth as an instance variable.\n",
    "        # Theory: Used to control recursion during tree building.\n",
    "        \n",
    "        self.tree = None\n",
    "        # Purpose: Initialize the tree structure as None.\n",
    "        # Theory: The tree will be built during fit, represented as a dictionary of nodes.\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        # Purpose: Compute the Gini index for a set of labels.\n",
    "        # Theory: Gini = 1 - Σ(p_i^2), where p_i is the proportion of class i. Measures node impurity, with 0 indicating a pure node.\n",
    "        \n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        # Purpose: Count occurrences of each class in y.\n",
    "        # Theory: np.unique returns unique classes and their counts, enabling proportion calculation.\n",
    "        \n",
    "        probs = counts / len(y)\n",
    "        # Purpose: Compute class proportions.\n",
    "        # Theory: Proportions are used to calculate Gini index.\n",
    "        \n",
    "        return 1 - np.sum(probs ** 2)\n",
    "        # Purpose: Calculate and return the Gini index.\n",
    "        # Theory: Lower Gini indicates purer nodes (e.g., Gini = 0 for single-class nodes).\n",
    "    \n",
    "    def best_split(self, X, y):\n",
    "        # Purpose: Find the best feature and threshold to split the data.\n",
    "        # Theory: Evaluates all possible splits to minimize weighted Gini index of child nodes.\n",
    "        \n",
    "        m, n = X.shape\n",
    "        # Purpose: Get number of samples (m) and features (n).\n",
    "        # Theory: Used to iterate over samples and features for split evaluation.\n",
    "        \n",
    "        if len(np.unique(y)) == 1:\n",
    "            return None, None\n",
    "        # Purpose: Return None if node is pure (single class).\n",
    "        # Theory: No split needed if all samples belong to one class.\n",
    "        \n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        # Purpose: Initialize variables to track the best split.\n",
    "        # Theory: Tracks the split with the lowest weighted Gini index.\n",
    "        \n",
    "        for feature in range(n):\n",
    "            # Purpose: Iterate over each feature.\n",
    "            # Theory: Each feature is tested for possible thresholds.\n",
    "            \n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Purpose: Get unique values of the feature as potential thresholds.\n",
    "            # Theory: Only unique values need testing to avoid redundant splits.\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Purpose: Test each threshold for the feature.\n",
    "                # Theory: Splits data into left (≤ threshold) and right (> threshold) subsets.\n",
    "                \n",
    "                left_idx = X[:, feature] <= threshold\n",
    "                right_idx = ~left_idx\n",
    "                # Purpose: Create boolean masks for left and right child nodes.\n",
    "                # Theory: Masks partition data based on the threshold.\n",
    "                \n",
    "                if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "                    continue\n",
    "                # Purpose: Skip splits that produce empty nodes.\n",
    "                # Theory: At least one sample is needed in each child node.\n",
    "                \n",
    "                gini_left = self.gini_index(y[left_idx])\n",
    "                gini_right = self.gini_index(y[right_idx])\n",
    "                # Purpose: Compute Gini index for left and right child nodes.\n",
    "                # Theory: Measures impurity of each child node.\n",
    "                \n",
    "                weighted_gini = (np.sum(left_idx) * gini_left + np.sum(right_idx) * gini_right) / m\n",
    "                # Purpose: Calculate weighted Gini index for the split.\n",
    "                # Theory: Weights by number of samples in each child node.\n",
    "                \n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                # Purpose: Update best split if weighted Gini is lower.\n",
    "                # Theory: Tracks the split that minimizes impurity.\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "        # Purpose: Return the best feature index and threshold.\n",
    "        # Theory: Used to construct the tree node or stop if no valid split is found.\n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        # Purpose: Recursively build the decision tree.\n",
    "        # Theory: Splits data at each node until stopping criteria (depth, purity) are met.\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        # Purpose: Get number of samples at current node.\n",
    "        # Theory: Used to check stopping conditions.\n",
    "        \n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1 or n_samples < 2:\n",
    "            # Purpose: Stop recursion if max depth reached, node is pure, or too few samples.\n",
    "            # Theory: Prevents overfitting and ensures valid leaf nodes.\n",
    "            \n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            # Purpose: Determine majority class for leaf node.\n",
    "            # Theory: Leaf nodes predict the most common class.\n",
    "            \n",
    "            return {'leaf': True, 'class': majority_class}\n",
    "            # Purpose: Return a leaf node with the predicted class.\n",
    "            # Theory: Leaf nodes store the class for predictions.\n",
    "        \n",
    "        feature, threshold = self.best_split(X, y)\n",
    "        # Purpose: Find the best split for the current node.\n",
    "        # Theory: Uses Gini index to select feature and threshold.\n",
    "        \n",
    "        if feature is None:\n",
    "            majority_class = np.bincount(y).argmax()\n",
    "            return {'leaf': True, 'class': majority_class}\n",
    "        # Purpose: Create a leaf if no valid split is found.\n",
    "        # Theory: Handles cases where no split reduces impurity.\n",
    "        \n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = ~left_idx\n",
    "        # Purpose: Split data into left and right child nodes.\n",
    "        # Theory: Partitions samples based on the threshold.\n",
    "        \n",
    "        left_tree = self.build_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_tree = self.build_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "        # Purpose: Recursively build left and right subtrees.\n",
    "        # Theory: Continues splitting until stopping criteria are met.\n",
    "        \n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "        # Purpose: Return an internal node with split details.\n",
    "        # Theory: Stores feature, threshold, and child nodes for traversal.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Purpose: Train the decision tree on input data and labels.\n",
    "        # Theory: Builds the tree by recursively splitting data using Gini index.\n",
    "        \n",
    "        self.tree = self.build_tree(X, y)\n",
    "        # Purpose: Build and store the tree.\n",
    "        # Theory: Starts recursive construction from the root node.\n",
    "        \n",
    "        return self\n",
    "        # Purpose: Return self for method chaining.\n",
    "        # Theory: Follows scikit-learn API convention.\n",
    "    \n",
    "    def predict_single(self, x, node):\n",
    "        # Purpose: Predict the class for a single sample by traversing the tree.\n",
    "        # Theory: Recursively navigates to a leaf node based on feature thresholds.\n",
    "        \n",
    "        if node['leaf']:\n",
    "            return node['class']\n",
    "        # Purpose: Return the class if at a leaf node.\n",
    "        # Theory: Leaf nodes store the majority class.\n",
    "        \n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self.predict_single(x, node['left'])\n",
    "        else:\n",
    "            return self.predict_single(x, node['right'])\n",
    "        # Purpose: Recursively traverse left or right child based on threshold.\n",
    "        # Theory: Follows the decision path to a leaf.\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Purpose: Predict classes for a batch of samples.\n",
    "        # Theory: Applies predict_single to each sample, handling batch inputs.\n",
    "        \n",
    "        return np.array([self.predict_single(x, self.tree) for x in X])\n",
    "        # Purpose: Return predictions as a NumPy array.\n",
    "        # Theory: Ensures compatibility with evaluation metrics.\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_idx = np.random.choice(100, 80, replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(100), train_idx)\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "# Purpose: Split data into 80% train and 20% test sets.\n",
    "# Theory: Train-test split evaluates model generalization. Random sampling ensures unbiased splits.\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "# Purpose: Initialize the decision tree with max depth of 3.\n",
    "# Theory: Limits complexity to prevent overfitting on small dataset.\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "# Purpose: Train the model on training data.\n",
    "# Theory: Builds the tree using recursive Gini-based splitting.\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "# Purpose: Generate predictions for test data.\n",
    "# Theory: Traverses the tree for each test sample to assign classes.\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "# Purpose: Compute accuracy as the proportion of correct predictions.\n",
    "# Theory: Accuracy = (correct predictions) / (total samples), a standard classification metric.\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# Purpose: Print the test accuracy.\n",
    "# Theory: Evaluates model performance, expected to be high due to separable data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
