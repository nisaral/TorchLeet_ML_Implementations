{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5d9794",
   "metadata": {},
   "source": [
    "Problem: Write a Custom Activation Function\n",
    "Problem Statement\n",
    "You are tasked with implementing a custom activation function in PyTorch that computes the following operation:\n",
    "\n",
    "\n",
    "Once implemented, this custom activation function will be used in a simple linear regression model.\n",
    "\n",
    "Requirements\n",
    "Custom Activation Function:\n",
    "\n",
    "Implement a class CustomActivationModel inheriting from torch.nn.Module.\n",
    "Define the forward method to compute the activation function ( \\text{tanh}(x) + x ).\n",
    "Integration with Linear Regression:\n",
    "\n",
    "Use the custom activation function in a simple linear regression model.\n",
    "The model should include:\n",
    "A single linear layer.\n",
    "The custom activation function applied to the output of the linear layer.\n",
    "Constraints\n",
    "The custom activation function should not have any learnable parameters.\n",
    "Ensure compatibility with PyTorch tensors for forward pass computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd51068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Purpose: Import the PyTorch library for tensor operations and neural network functionality.\n",
    "# Theory: PyTorch provides tensor computations (like NumPy but with GPU support) and autograd for automatic differentiation, essential for building and training neural networks.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules from PyTorch, including base class nn.Module and layers like nn.Linear.\n",
    "# Theory: nn.Module is the foundation for defining custom neural network models, managing parameters and forward passes.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Purpose: Import optimization algorithms like SGD for updating model parameters during training.\n",
    "# Theory: Optimizers use gradients computed by autograd to minimize the loss function via methods like gradient descent.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix the random seed to ensure consistent random number generation across runs.\n",
    "# Theory: Setting a seed ensures reproducibility of random operations (e.g., data generation, weight initialization), critical for debugging and comparing results.\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.rand(100, 1) * 10\n",
    "# Purpose: Create a tensor of 100 random input points between 0 and 10, with shape [100, 1].\n",
    "# Theory: torch.rand generates values from a uniform distribution [0, 1). Scaling by 10 maps to [0, 10). The shape [100, 1] represents 100 samples with 1 feature each.\n",
    "\n",
    "y = 2 * X + 3 + torch.randn(100, 1)\n",
    "# Purpose: Generate target values using a linear relationship y = 2x + 3 with added Gaussian noise.\n",
    "# Theory: The linear component (2 * X + 3) simulates a true linear relationship. torch.randn adds noise from a standard normal distribution (mean 0, std 1), mimicking real-world data imperfections.\n",
    "\n",
    "# Define the Custom Activation Model\n",
    "class CustomActivationModel(nn.Module):\n",
    "    # Purpose: Define a custom neural network model by subclassing nn.Module.\n",
    "    # Theory: nn.Module provides infrastructure for parameter management, forward pass definition, and autograd integration.\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomActivationModel, self).__init__()\n",
    "        # Purpose: Initialize the parent nn.Module class to set up the model.\n",
    "        # Theory: super() ensures proper initialization of nn.Module, registering parameters and enabling methods like parameters().\n",
    "        \n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        # Purpose: Create a linear layer that maps a single input feature to a single output (z = wx + b).\n",
    "        # Theory: nn.Linear applies a linear transformation z = wx + b, where w and b are learnable parameters initialized randomly (Xavier initialization by default in PyTorch).\n",
    "    \n",
    "    def custom_activation(self, x):\n",
    "        # Purpose: Implement the custom activation function f(x) = tanh(x) + x.\n",
    "        # Theory: The activation combines the non-linear tanh(x) (bounded in [-1, 1]) with the linear x, creating a function that grows linearly but is modulated by tanh’s non-linearity.\n",
    "        \n",
    "        return torch.tanh(x) + x\n",
    "        # Purpose: Compute tanh(x) + x element-wise on the input tensor.\n",
    "        # Theory: torch.tanh applies the hyperbolic tangent function element-wise. The addition (+) broadcasts and adds x, requiring compatible tensor shapes. The operation is differentiable, enabling autograd to compute gradients.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Define the forward pass of the model, specifying how input x is transformed to output.\n",
    "        # Theory: The forward method is called when the model is invoked (e.g., model(X)). It defines the computational graph for autograd.\n",
    "        \n",
    "        z = self.linear(x)\n",
    "        # Purpose: Apply the linear transformation z = wx + b to input x.\n",
    "        # Theory: For input x of shape [batch_size, 1], nn.Linear computes z = x @ w^T + b, where w is [1, 1] and b is [1], producing z of shape [batch_size, 1].\n",
    "        \n",
    "        return self.custom_activation(z)\n",
    "        # Purpose: Apply the custom activation function to the linear output.\n",
    "        # Theory: Computes f(z) = tanh(z) + z, introducing non-linearity. The output shape remains [batch_size, 1]. Autograd tracks this operation for backpropagation.\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CustomActivationModel()\n",
    "# Purpose: Create an instance of the custom model.\n",
    "# Theory: Instantiates the model, initializing the linear layer’s parameters (w, b) with random values. These parameters are registered with autograd for gradient tracking.\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Purpose: Define the Mean Squared Error (MSE) loss function.\n",
    "# Theory: MSE loss computes L = (1/n) * sum((y_pred - y)^2), measuring the squared difference between predictions and targets. It’s suitable for regression tasks.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# Purpose: Initialize Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01.\n",
    "# Theory: SGD updates parameters using the rule θ = θ - η * ∇L, where η is the learning rate and ∇L is the gradient. model.parameters() provides w and b from the linear layer.\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "# Purpose: Set the number of training iterations (epochs) to 1000.\n",
    "# Theory: Each epoch processes the entire dataset once, updating parameters to minimize loss. 1000 epochs is sufficient for convergencekkeyuh on this simple dataset.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Purpose: Iterate over the dataset for the specified number of epochs.\n",
    "    # Theory: Training involves repeated forward and backward passes to optimize parameters.\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    # Purpose: Compute model predictions by passing input X through the model.\n",
    "    # Theory: Calls the forward method, computing z = Xw + b, then y_pred = tanh(z) + z. X [100, 1] produces predictions [100, 1].\n",
    "    \n",
    "    loss = criterion(predictions, y)\n",
    "    # Purpose: Calculate the MSE loss between predictions and true targets.\n",
    "    # Theory: Computes L = (1/100) * sum((predictions - y)^2). Both tensors have shape [100, 1], ensuring compatibility.\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    # Purpose: Reset gradients of all model parameters to zero.\n",
    "    # Theory: Gradients accumulate by default in PyTorch. Zeroing prevents mixing gradients from previous iterations, ensuring correct updates.\n",
    "    \n",
    "    loss.backward()\n",
    "    # Purpose: Compute gradients of the loss with respect to model parameters (w, b).\n",
    "    # Theory: Autograd backpropagates through the computational graph: loss → activation → linear layer. Computes ∂L/∂w and ∂L/∂b using the chain rule.\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Purpose: Update model parameters using the computed gradients.\n",
    "    # Theory: SGD applies θ = θ - η * ∇L for each parameter (w, b), where η = 0.01. This minimizes the loss.\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Purpose: Print training progress every 100 epochs.\n",
    "        # Theory: Monitoring loss helps assess convergence and detect issues like overfitting or underfitting.\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        # Purpose: Display the current epoch and loss value.\n",
    "        # Theory: loss.item() extracts the scalar loss value from the tensor. Formatting to 4 decimal places improves readability.\n",
    "\n",
    "# Display learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "# Purpose: Retrieve the learned weight and bias from the linear layer.\n",
    "# Theory: model.parameters() yields an iterator of tensors (w [1, 1], b [1]). Unpacking assigns them to w and b.\n",
    "\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "# Purpose: Print the learned weight and bias values.\n",
    "# Theory: w.item() and b.item() convert single-element tensors to scalars. Due to the non-linear activation, these may not match the true values (2, 3).\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "# Purpose: Create a test input tensor with two values (4.0, 7.0).\n",
    "# Theory: Test inputs have shape [2, 1], matching the model’s input requirement. Used to evaluate model generalization.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for inference to save memory and computation.\n",
    "    # Theory: Gradient tracking is unnecessary during inference, as no parameters are updated. torch.no_grad() ensures this.\n",
    "    \n",
    "    predictions = model(X_test)\n",
    "    # Purpose: Compute predictions for test inputs.\n",
    "    # Theory: Passes X_test through the model (linear → activation), producing predictions of shape [2, 1].\n",
    "    \n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "    # Purpose: Print test inputs and their predictions.\n",
    "    # Theory: .tolist() converts tensors to Python lists for readable output. Predictions reflect the non-linear transformation f(z) = tanh(z) + z."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
