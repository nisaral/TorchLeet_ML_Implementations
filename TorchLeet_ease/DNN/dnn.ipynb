{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d5fc41",
   "metadata": {},
   "source": [
    "Problem: Implement a Deep Neural Network\n",
    "Problem Statement\n",
    "You are tasked with constructing a Deep Neural Network (DNN) model to solve a regression task using PyTorch. The objective is to predict target values from synthetic data exhibiting a non-linear relationship.\n",
    "\n",
    "Requirements\n",
    "Implement the DNNModel class that satisfies the following criteria:\n",
    "\n",
    "Model Definition:\n",
    "The model should have:\n",
    "An input layer connected to a hidden layer.\n",
    "A ReLU activation function for non-linearity.\n",
    "An output layer with a single unit for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Set a random seed for reproducibility of synthetic data.\n",
    "# Theory: Fixing the seed ensures consistent random number generation, aligning with previous problems (e.g., linear regression, Huber Loss).\n",
    "\n",
    "X = torch.rand(100, 2) * 10\n",
    "# Purpose: Create 100 data points with 2 features, values between 0 and 10.\n",
    "# Theory: torch.rand generates uniform random numbers in [0, 1). Scaling by 10 maps to [0, 10). Shape [100, 2] represents 100 samples with 2 features.\n",
    "\n",
    "y = (X[:, 0] + X[:, 1] * 2).unsqueeze(1) + torch.randn(100, 1)\n",
    "# Purpose: Generate target values using y = x1 + 2*x2 + noise, with shape [100, 1].\n",
    "# Theory: The linear term (x1 + 2*x2) defines a relationship, with Gaussian noise (mean 0, std 1) added via torch.randn. unsqueeze(1) ensures y is [100, 1] for regression.\n",
    "\n",
    "# Define the Deep Neural Network Model\n",
    "class DNNModel(nn.Module):\n",
    "    # Purpose: Define a deep neural network for regression by subclassing nn.Module.\n",
    "    # Theory: nn.Module provides infrastructure for layer management, parameter registration, and autograd integration.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Purpose: Initialize the DNN with multiple fully connected layers.\n",
    "        # Theory: Defines the architecture: input (2 features) → hidden layers → output (1 value). Layers include weights and biases.\n",
    "        \n",
    "        super(DNNModel, self).__init__()\n",
    "        # Purpose: Call the parent nn.Module constructor to set up the module.\n",
    "        # Theory: Ensures proper initialization, enabling parameter tracking and methods like parameters().\n",
    "        \n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        # Purpose: Create the first fully connected layer mapping 2 inputs to 16 hidden units.\n",
    "        # Theory: Applies z1 = W1*x + b1, where W1 is [16, 2], b1 is [16]. Initialized with Xavier initialization by default.\n",
    "        \n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        # Purpose: Create the second fully connected layer mapping 16 units to 8 units.\n",
    "        # Theory: Applies z2 = W2*h1 + b2, where W2 is [8, 16], b2 is [8]. Reduces dimensionality while adding complexity.\n",
    "        \n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        # Purpose: Create the output layer mapping 8 units to 1 output.\n",
    "        # Theory: Applies y = W3*h2 + b3, where W3 is [1, 8], b3 is [1]. Produces a single regression value.\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # Purpose: Define the ReLU activation function for non-linearity.\n",
    "        # Theory: ReLU(x) = max(0, x) introduces sparsity and prevents vanishing gradients, applied after each hidden layer.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Define the forward pass, specifying how input x is transformed to output.\n",
    "        # Theory: The forward method builds the computational graph for autograd, computing predictions through layers and activations.\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        # Purpose: Apply first linear layer and ReLU activation.\n",
    "        # Theory: Computes h1 = ReLU(W1*x + b1). Input x [batch_size, 2] produces h1 [batch_size, 16].\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        # Purpose: Apply second linear layer and ReLU activation.\n",
    "        # Theory: Computes h2 = ReLU(W2*h1 + b2). Input h1 [batch_size, 16] produces h2 [batch_size, 8].\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        # Purpose: Apply output layer to produce predictions.\n",
    "        # Theory: Computes y = W3*h2 + b3. Input h2 [batch_size, 8] produces y [batch_size, 1]. No activation for regression.\n",
    "        \n",
    "        return x\n",
    "        # Purpose: Return the final predictions.\n",
    "        # Theory: Output tensor [batch_size, 1] represents predicted values for regression.\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = DNNModel()\n",
    "# Purpose: Create an instance of the DNN model.\n",
    "# Theory: Instantiates the model, initializing weights and biases randomly with Xavier initialization. Parameters are tracked by autograd.\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Purpose: Define the Mean Squared Error (MSE) loss function.\n",
    "# Theory: MSE computes L = (1/n) * sum((y_pred - y_true)^2), suitable for regression. Measures squared differences between predictions and targets.\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Purpose: Initialize the Adam optimizer with a learning rate of 0.01.\n",
    "# Theory: Adam adapts learning rates using momentum (β1=0.9) and squared gradients (β2=0.999). model.parameters() provides all weights and biases.\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "# Purpose: Set the number of training iterations to 1000 epochs.\n",
    "# Theory: Each epoch processes the entire dataset, updating parameters to minimize loss. 1000 epochs ensures convergence for this dataset.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Purpose: Iterate over the dataset for the specified number of epochs.\n",
    "    # Theory: Training involves repeated forward and backward passes to optimize model parameters.\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    # Purpose: Compute model predictions by passing input X through the model.\n",
    "    # Theory: Calls the forward method, computing y_pred = DNN(X). X [100, 2] produces predictions [100, 1].\n",
    "    \n",
    "    loss = criterion(predictions, y)\n",
    "    # Purpose: Calculate the MSE loss between predictions and true targets.\n",
    "    # Theory: Computes L = (1/100) * sum((predictions - y)^2). Both tensors are [100, 1], ensuring compatibility.\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    # Purpose: Reset gradients of all model parameters to zero.\n",
    "    # Theory: Gradients accumulate by default in PyTorch. Zeroing prevents mixing gradients from previous iterations.\n",
    "    \n",
    "    loss.backward()\n",
    "    # Purpose: Compute gradients of the loss with respect to model parameters.\n",
    "    # Theory: Autograd backpropagates through the network (output → hidden → input), computing ∂L/∂W and ∂L/∂b for all layers.\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Purpose: Update model parameters using the computed gradients.\n",
    "    # Theory: Adam applies adaptive updates to weights and biases, minimizing the loss.\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Purpose: Print training progress every 100 epochs.\n",
    "        # Theory: Monitoring loss helps assess convergence and detect issues like overfitting or learning rate problems.\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        # Purpose: Display the current epoch and loss value.\n",
    "        # Theory: loss.item() extracts the scalar loss value. Formatting to 4 decimal places improves readability.\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0, 3.0], [7.0, 8.0]])\n",
    "# Purpose: Create a test input tensor with two samples.\n",
    "# Theory: Test inputs [2, 2] match the model’s input shape, used to evaluate generalization. Expected outputs are ~10 and ~23 based on y = x1 + 2*x2.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for inference to save memory and computation.\n",
    "    # Theory: Gradient tracking is unnecessary during inference, as no parameters are updated.\n",
    "    \n",
    "    predictions = model(X_test)\n",
    "    # Purpose: Compute predictions for test inputs.\n",
    "    # Theory: Passes X_test through the model, producing predictions [2, 1].\n",
    "    \n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "    # Purpose: Print test inputs and their predictions.\n",
    "    # Theory: .tolist() converts tensors to Python lists for readable output. Predictions should approximate true values plus noise."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
