{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40d770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69a6dfc5",
   "metadata": {},
   "source": [
    "Problem: Implement Custom Loss Function (Huber Loss)\n",
    "Problem Statement\n",
    "You are tasked with implementing the Huber Loss as a custom loss function in PyTorch. The Huber loss is a robust loss function used in regression tasks, less sensitive to outliers than Mean Squared Error (MSE). It transitions between L2 loss (squared error) and L1 loss (absolute error) based on a threshold parameter \n",
    ".\n",
    "\n",
    "The Huber loss is mathematically defined as:\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    " y is the true value,\n",
    " y1 is the predicted value,\n",
    " delta is a threshold parameter that controls the transition between L1 and L2 loss.\n",
    "Requirements\n",
    "Custom Loss Function:\n",
    "\n",
    "Implement a class HuberLoss inheriting from torch.nn.Module.\n",
    "Define the forward method to compute the Huber loss as per the formula.\n",
    "Usage in a Regression Model:\n",
    "\n",
    "Integrate the custom loss function into a regression training pipeline.\n",
    "Use it to compute and optimize the loss during model training.\n",
    "Constraints\n",
    "The implementation must handle both scalar and batch inputs for \n",
    " (true values) and \n",
    " (predicted values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8470cecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.8869\n",
      "Epoch [200/1000], Loss: 0.7855\n",
      "Epoch [300/1000], Loss: 0.6945\n",
      "Epoch [400/1000], Loss: 0.6134\n",
      "Epoch [500/1000], Loss: 0.5433\n",
      "Epoch [600/1000], Loss: 0.4861\n",
      "Epoch [700/1000], Loss: 0.4404\n",
      "Epoch [800/1000], Loss: 0.4045\n",
      "Epoch [900/1000], Loss: 0.3767\n",
      "Epoch [1000/1000], Loss: 0.3551\n",
      "Learned weight: 2.0713, Learned bias: 2.4650\n",
      "Predictions for [[4.0], [7.0]]: [[10.750251770019531], [16.964160919189453]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10\n",
    "y = 2 * X + 3 + torch.randn(100, 1)\n",
    "# Define the Huber Loss\n",
    "class HuberLoss(nn.Module):\n",
    "    # Purpose: Define a custom loss function by subclassing nn.Module.\n",
    "    # Theory: nn.Module allows defining custom operations with forward passes, integrating with PyTorch’s autograd for gradient computation.\n",
    "    \n",
    "    def __init__(self, delta=1.0):\n",
    "        # Purpose: Initialize the Huber Loss with a threshold parameter delta.\n",
    "        # Theory: delta controls the transition between L2 (squared) and L1 (absolute) loss. A common default is 1.0, balancing robustness and smoothness.\n",
    "        \n",
    "        super(HuberLoss, self).__init__()\n",
    "        # Purpose: Call the parent nn.Module constructor to set up the module.\n",
    "        # Theory: super() ensures proper initialization, enabling features like parameter registration (though no learnable parameters are used here).\n",
    "        \n",
    "        self.delta = delta\n",
    "        # Purpose: Store the delta parameter as an instance variable.\n",
    "        # Theory: delta is a hyperparameter, not a learnable parameter, so it’s stored as a Python attribute, not a torch.Parameter.\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Purpose: Compute the Huber Loss for predicted and true values.\n",
    "        # Theory: The forward method defines the loss computation, called when the loss is invoked (e.g., criterion(predictions, y)). It must handle batch inputs.\n",
    "        \n",
    "        residual = torch.abs(y_pred - y_true)\n",
    "        # Purpose: Compute the absolute error |y_pred - y_true| element-wise.\n",
    "        # Theory: The residual determines whether each error falls in the L2 (<= delta) or L1 (> delta) regime. torch.abs ensures non-negative values.\n",
    "        \n",
    "        is_small_error = residual <= self.delta\n",
    "        # Purpose: Create a boolean tensor identifying errors where |y_pred - y_true| <= delta.\n",
    "        # Theory: Logical indexing allows separating L2 and L1 loss cases. The tensor has the same shape as residual, with True for small errors.\n",
    "        \n",
    "        squared_loss = 0.5 * residual ** 2\n",
    "        # Purpose: Compute the L2 loss term (0.5 * (y_pred - y_true)^2) for all elements.\n",
    "        # Theory: For small errors, this term is used directly. The factor 0.5 ensures the gradient is -(y_pred - y_true), matching MSE behavior.\n",
    "        \n",
    "        linear_loss = self.delta * residual - 0.5 * self.delta ** 2\n",
    "        # Purpose: Compute the L1 loss term (delta * |y_pred - y_true| - 0.5 * delta^2) for all elements.\n",
    "        # Theory: For large errors, this term is used. The subtraction ensures continuity at |y_pred - y_true| = delta, making the loss differentiable.\n",
    "        \n",
    "        loss = torch.where(is_small_error, squared_loss, linear_loss)\n",
    "        # Purpose: Select squared_loss for small errors and linear_loss for large errors.\n",
    "        # Theory: torch.where(condition, x, y) applies x where condition is True, else y. This implements the piecewise Huber Loss definition.\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "        # Purpose: Average the loss over the batch to produce a scalar loss.\n",
    "        # Theory: Reducing the loss to a scalar (via mean) is standard for optimization, ensuring gradients are computed for the entire batch.\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    # Purpose: Define a simple linear regression model by subclassing nn.Module.\n",
    "    # Theory: nn.Module provides infrastructure for defining neural network architectures, including parameter management and forward passes.\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Purpose: Initialize the parent nn.Module class to set up the model.\n",
    "        # Theory: super() ensures proper initialization, registering parameters and enabling methods like parameters().\n",
    "        \n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        # Purpose: Create a linear layer mapping 1 input feature to 1 output (y = wx + b).\n",
    "        # Theory: nn.Linear applies a linear transformation y = wx + b, with learnable parameters w [1, 1] and b [1], initialized using Xavier initialization.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Define the forward pass, specifying how input x is transformed to output.\n",
    "        # Theory: The forward method is called when the model is invoked (e.g., model(X)), building the computational graph for autograd.\n",
    "        \n",
    "        return self.linear(x)\n",
    "        # Purpose: Apply the linear transformation to input x.\n",
    "        # Theory: Computes y = x @ w^T + b, where x is [batch_size, 1], producing output of shape [batch_size, 1].\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "# Purpose: Create an instance of the linear regression model.\n",
    "# Theory: Instantiates the model, initializing w and b randomly. These parameters are registered with autograd for gradient tracking.\n",
    "\n",
    "criterion = HuberLoss(delta=1.0)\n",
    "# Purpose: Initialize the Huber Loss with delta = 1.0.\n",
    "# Theory: delta = 1.0 is a common choice, balancing L2 loss for small errors and L1 loss for outliers. The loss module is used like a function during training.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# Purpose: Initialize Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01.\n",
    "# Theory: SGD updates parameters using θ = θ - η * ∇L, where η is the learning rate. model.parameters() provides w and b from the linear layer.\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "# Purpose: Set the number of training iterations to 1000 epochs.\n",
    "# Theory: Each epoch processes the entire dataset, updating parameters to minimize loss. 1000 epochs allows convergence for this simple model.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Purpose: Iterate over the dataset for the specified number of epochs.\n",
    "    # Theory: Training involves repeated forward and backward passes to optimize parameters.\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    # Purpose: Compute model predictions by passing input X through the model.\n",
    "    # Theory: Calls the forward method, computing y_pred = Xw + b. X [100, 1] produces predictions [100, 1].\n",
    "    \n",
    "    loss = criterion(predictions, y)\n",
    "    # Purpose: Calculate the Huber Loss between predictions and true targets.\n",
    "    # Theory: Computes the piecewise loss for each sample, averaging over the batch. Both tensors are [100, 1], ensuring compatibility.\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    # Purpose: Reset gradients of all model parameters to zero.\n",
    "    # Theory: Gradients accumulate by default in PyTorch. Zeroing prevents mixing gradients from previous iterations.\n",
    "    \n",
    "    loss.backward()\n",
    "    # Purpose: Compute gradients of the loss with respect to model parameters (w, b).\n",
    "    # Theory: Autograd backpropagates through the loss → linear layer, computing ∂L/∂w and ∂L/∂b using the chain rule.\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Purpose: Update model parameters using the computed gradients.\n",
    "    # Theory: SGD applies θ = θ - η * ∇L for each parameter, minimizing the loss.\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Purpose: Print training progress every 100 epochs.\n",
    "        # Theory: Monitoring loss helps assess convergence and detect issues like underfitting or learning rate problems.\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        # Purpose: Display the current epoch and loss value.\n",
    "        # Theory: loss.item() extracts the scalar loss value. Formatting to 4 decimal places improves readability.\n",
    "\n",
    "# Display learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "# Purpose: Retrieve the learned weight and bias from the linear layer.\n",
    "# Theory: model.parameters() yields an iterator of tensors (w [1, 1], b [1]). Unpacking assigns them to w and b.\n",
    "\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "# Purpose: Print the learned weight and bias values.\n",
    "# Theory: w.item() and b.item() convert tensors to scalars. With Huber Loss, parameters should approximate true values (2, 3), as it’s robust to noise.\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "# Purpose: Create a test input tensor with values 4.0 and 7.0.\n",
    "# Theory: Test inputs [2, 1] match the model’s input shape, used to evaluate generalization.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for inference to save memory and computation.\n",
    "    # Theory: Gradient tracking is unnecessary during inference, as no parameters are updated.\n",
    "    \n",
    "    predictions = model(X_test)\n",
    "    # Purpose: Compute predictions for test inputs.\n",
    "    # Theory: Passes X_test through the model, producing predictions [2, 1].\n",
    "    \n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "    # Purpose: Print test inputs and their predictions.\n",
    "    # Theory: .tolist() converts tensors to Python lists for readable output. Predictions should approximate 2 * x + 3 (e.g., 11 for x=4, 17 for x=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23d853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
