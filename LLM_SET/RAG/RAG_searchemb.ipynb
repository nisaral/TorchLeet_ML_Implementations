{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e653e42a",
   "metadata": {},
   "source": [
    "# RAG Search of Embeddings from a Set of Reviews\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Title**: Implement Retrieval-Augmented Generation (RAG) Search of Embeddings for Reviews\n",
    "\n",
    "**Description**: You are tasked with implementing a Retrieval-Augmented Generation (RAG) system that searches a set of review texts using embeddings to retrieve the most relevant reviews for a given query. The system should encode reviews into dense embeddings using a simple neural network, compute similarity between a query embedding and review embeddings, and return the top-k most similar reviews. This mimics the retrieval component of RAG used in LLMs to enhance generation with relevant context. Use PyTorch to generate embeddings and NumPy for similarity computation, and test on a synthetic dataset of reviews.\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "### Embedding Generation\n",
    "\n",
    "For a review text r_i, tokenize using a simple word-based tokenizer (or reuse BPE from the previous problem).\n",
    "\n",
    "Map tokens to embeddings via a neural network:\n",
    "```\n",
    "e_i = f(r_i; θ)\n",
    "```\n",
    "where e_i ∈ R^d is the embedding (e.g., d = 64).\n",
    "\n",
    "### Similarity Search\n",
    "\n",
    "For a query text q, compute its embedding:\n",
    "```\n",
    "e_q = f(q; θ)\n",
    "```\n",
    "\n",
    "Compute cosine similarity between query and review embeddings:\n",
    "```\n",
    "cosine_similarity(e_q, e_i) = (e_q · e_i) / (||e_q|| ||e_i||)\n",
    "```\n",
    "\n",
    "Return the top-k reviews with highest similarity scores.\n",
    "\n",
    "### Training\n",
    "\n",
    "Train the embedding model to maximize similarity between related reviews and minimize it for unrelated ones, using a contrastive loss:\n",
    "\n",
    "```\n",
    "L = -log(exp(cosine_similarity(e_i, e_j) / τ) / Σ_{k≠i} exp(cosine_similarity(e_i, e_k) / τ))\n",
    "```\n",
    "\n",
    "where (r_i, r_j) are positive pairs (similar reviews), τ is a temperature parameter.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### RAGSearch Class Implementation\n",
    "\n",
    "Implement a `RAGSearch` class with methods for:\n",
    "\n",
    "- **train**: Train a neural network to generate embeddings from tokenized reviews\n",
    "- **encode**: Convert text to embeddings\n",
    "- **search**: Retrieve top-k reviews based on cosine similarity\n",
    "\n",
    "### Dataset and Training\n",
    "\n",
    "- Use a synthetic dataset of 100 short review texts (e.g., product or movie reviews)\n",
    "- Tokenize using a simple word-based tokenizer or BPE (from previous problem)\n",
    "- Train the embedding model with contrastive loss on labeled pairs (e.g., similar/dissimilar reviews)\n",
    "- Test retrieval on sample queries, returning top-5 reviews\n",
    "- Provide detailed **Purpose** and **Theory** comments for each line of code\n",
    "\n",
    "## Constraints\n",
    "\n",
    "- Use PyTorch for embedding generation, NumPy for similarity computation\n",
    "- No external libraries like transformers or sentence-transformers\n",
    "- Embedding dimension: d = 64\n",
    "- Handle batch processing for efficiency\n",
    "- Train for 100 epochs with Adam optimizer (learning rate 0.001)\n",
    "- Use cosine similarity for retrieval\n",
    "\n",
    "## Synthetic Dataset\n",
    "\n",
    "### Review Data\n",
    "- **Reviews**: 100 short texts (e.g., \"Great product, fast delivery\", \"Poor quality, broke quickly\")\n",
    "- **Positive Pairs**: Pairs of reviews with similar sentiment or topic (e.g., both positive about delivery)\n",
    "- **Query Examples**: \"Amazing product\", \"Bad service\", \"Fast shipping\"\n",
    "- **Vocabulary**: Generated from the corpus (e.g., using BPE or word splitting)\n",
    "- **Test Queries**: 3 queries to retrieve top-5 similar reviews\n",
    "\n",
    "### Data Structure\n",
    "```python\n",
    "reviews = [\n",
    "    \"Great product, fast delivery\",\n",
    "    \"Poor quality, broke quickly\",\n",
    "    \"Amazing service, highly recommend\",\n",
    "    # ... 97 more reviews\n",
    "]\n",
    "\n",
    "positive_pairs = [\n",
    "    (0, 2),  # Both positive reviews\n",
    "    (1, 5),  # Both negative reviews\n",
    "    # ... more pairs\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"Amazing product\",\n",
    "    \"Bad service\", \n",
    "    \"Fast shipping\"\n",
    "]\n",
    "```\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### RAGSearch Class Structure\n",
    "\n",
    "```python\n",
    "class RAGSearch:\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Purpose: Initialize RAG search system with embedding model\n",
    "        Theory: Creates neural network for text-to-embedding mapping\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer\n",
    "        # Initialize embedding model (neural network)\n",
    "        # Initialize similarity computation utilities\n",
    "        \n",
    "    def train(self, reviews, positive_pairs, epochs=100, lr=0.001):\n",
    "        \"\"\"\n",
    "        Purpose: Train embedding model using contrastive loss\n",
    "        Theory: Learn embeddings that maximize similarity for related reviews\n",
    "        \"\"\"\n",
    "        # Setup optimizer and loss function\n",
    "        # Training loop with contrastive loss\n",
    "        # Track loss convergence\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Purpose: Convert text to dense embedding vector\n",
    "        Theory: Apply trained neural network to tokenized text\n",
    "        \"\"\"\n",
    "        # Tokenize input text\n",
    "        # Pass through embedding model\n",
    "        # Return normalized embedding\n",
    "        \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Purpose: Retrieve top-k most similar reviews for query\n",
    "        Theory: Compute cosine similarity and rank results\n",
    "        \"\"\"\n",
    "        # Encode query to embedding\n",
    "        # Compute similarities with all review embeddings\n",
    "        # Return top-k results with scores\n",
    "```\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "```python\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Purpose: Neural network for text-to-embedding transformation\n",
    "        Theory: Simple feedforward network with word embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Word embedding layer\n",
    "        # Hidden layers\n",
    "        # Output projection to embedding dimension\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Purpose: Forward pass through embedding model\n",
    "        Theory: Transform token IDs to dense embeddings\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        # Apply neural network layers\n",
    "        # Return normalized embeddings\n",
    "```\n",
    "\n",
    "### Tokenization Strategy\n",
    "\n",
    "```python\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Purpose: Simple word-based tokenizer\n",
    "        Theory: Convert text to token IDs for neural network input\n",
    "        \"\"\"\n",
    "        # Initialize vocabulary\n",
    "        # Setup encoding/decoding methods\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Purpose: Convert text to token IDs\n",
    "        Theory: Split text and map to vocabulary indices\n",
    "        \"\"\"\n",
    "        # Tokenize text\n",
    "        # Convert to numerical IDs\n",
    "        # Return token sequence\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Purpose: Build vocabulary from training corpus\n",
    "        Theory: Extract unique tokens and assign IDs\n",
    "        \"\"\"\n",
    "        # Extract all tokens from texts\n",
    "        # Create word-to-ID mapping\n",
    "        # Handle unknown tokens\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "### Training Progress\n",
    "```\n",
    "Epoch [10/100], Loss: 0.8234\n",
    "Epoch [20/100], Loss: 0.7456\n",
    "...\n",
    "Epoch [100/100], Loss: 0.0921\n",
    "```\n",
    "\n",
    "### Retrieval Results\n",
    "```\n",
    "Query: \"Amazing product\"\n",
    "Top-5 Reviews:\n",
    "1. \"Great product, fast delivery\" (Similarity: 0.9512)\n",
    "2. \"Awesome item, highly recommend\" (Similarity: 0.9278)\n",
    "3. \"Excellent quality, worth buying\" (Similarity: 0.9156)\n",
    "4. \"Outstanding product, great value\" (Similarity: 0.9034)\n",
    "5. \"Fantastic purchase, very satisfied\" (Similarity: 0.8967)\n",
    "```\n",
    "\n",
    "### Model Performance\n",
    "- **Trained Model**: Embedding model producing 64-dimensional embeddings\n",
    "- **Loss**: Decreases from ~1.0 to ~0.1 over 100 epochs\n",
    "- **Retrieval Results**: Top-5 reviews with similarity scores (e.g., 0.95, 0.90, ...)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Training Metrics\n",
    "- **Contrastive Loss Convergence**: Monitor loss decrease over epochs\n",
    "- **Embedding Quality**: Check that similar reviews have high cosine similarity (~0.9)\n",
    "\n",
    "### Retrieval Metrics\n",
    "- **Retrieval Accuracy**: Top-k reviews should match query sentiment/topic\n",
    "- **Similarity Score Distribution**: Relevant results should have high scores (>0.8)\n",
    "- **Ranking Quality**: Most relevant reviews should rank highest\n",
    "\n",
    "### Evaluation Framework\n",
    "```python\n",
    "def evaluate_retrieval(rag_search, test_queries, ground_truth):\n",
    "    \"\"\"\n",
    "    Purpose: Evaluate retrieval performance\n",
    "    Theory: Measure accuracy and relevance of retrieved results\n",
    "    \"\"\"\n",
    "    # For each test query\n",
    "    # Retrieve top-k results\n",
    "    # Compare with ground truth\n",
    "    # Calculate evaluation metrics\n",
    "```\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### 1. Data Preparation\n",
    "```python\n",
    "# Generate synthetic review dataset\n",
    "# Create positive pairs for contrastive learning\n",
    "# Build vocabulary from review corpus\n",
    "# Prepare test queries\n",
    "```\n",
    "\n",
    "### 2. Model Implementation\n",
    "```python\n",
    "# Implement SimpleTokenizer class\n",
    "# Implement EmbeddingModel neural network\n",
    "# Implement RAGSearch main class\n",
    "# Setup training infrastructure\n",
    "```\n",
    "\n",
    "### 3. Training Pipeline\n",
    "```python\n",
    "# Initialize RAGSearch system\n",
    "# Train embedding model with contrastive loss\n",
    "# Monitor loss convergence\n",
    "# Save trained model\n",
    "```\n",
    "\n",
    "### 4. Evaluation and Testing\n",
    "```python\n",
    "# Test retrieval on sample queries\n",
    "# Evaluate embedding quality\n",
    "# Measure retrieval accuracy\n",
    "# Generate performance reports\n",
    "```\n",
    "\n",
    "## Key Technical Details\n",
    "\n",
    "### Contrastive Loss Implementation\n",
    "- Use temperature parameter τ = 0.1 for loss scaling\n",
    "- Implement efficient batch processing for positive/negative pairs\n",
    "- Handle numerical stability in softmax computation\n",
    "\n",
    "### Cosine Similarity Computation\n",
    "- Normalize embeddings before similarity computation\n",
    "- Use efficient NumPy operations for batch similarity\n",
    "- Handle edge cases (zero vectors, identical embeddings)\n",
    "\n",
    "### Memory and Efficiency\n",
    "- Batch processing for training efficiency\n",
    "- Precompute review embeddings for fast retrieval\n",
    "- Use appropriate data structures for similarity search\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize RAG search system\n",
    "rag_search = RAGSearch(vocab_size=1000, embedding_dim=64)\n",
    "\n",
    "# Train on review dataset\n",
    "rag_search.train(reviews, positive_pairs, epochs=100)\n",
    "\n",
    "# Search for similar reviews\n",
    "results = rag_search.search(\"Amazing product\", top_k=5)\n",
    "\n",
    "# Display results\n",
    "for i, (review, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. \\\"{review}\\\" (Similarity: {score:.4f})\")\n",
    "```\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. **RAGSearch Class**: Complete implementation with train, encode, and search methods\n",
    "2. **EmbeddingModel**: Neural network for text-to-embedding transformation\n",
    "3. **SimpleTokenizer**: Text tokenization and vocabulary management\n",
    "4. **Training Pipeline**: Contrastive loss training with convergence monitoring\n",
    "5. **Evaluation Framework**: Retrieval accuracy and embedding quality assessment\n",
    "6. **Synthetic Dataset**: 100 reviews with positive pairs and test queries\n",
    "7. **Documentation**: Detailed code comments explaining purpose and theory\n",
    "8. **Results Analysis**: Performance metrics and retrieval examples\n",
    "\n",
    "## Advanced Features (Optional)\n",
    "\n",
    "### Enhanced Similarity Metrics\n",
    "- Implement additional similarity measures (dot product, Euclidean distance)\n",
    "- Compare retrieval performance across different metrics\n",
    "\n",
    "### Improved Training\n",
    "- Add negative sampling strategies\n",
    "- Implement learning rate scheduling\n",
    "- Add validation set monitoring\n",
    "\n",
    "### Scalability Improvements\n",
    "- Implement approximate nearest neighbor search\n",
    "- Add embedding caching mechanisms\n",
    "- Support for incremental index updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1676e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 4.6349\n",
      "Epoch [20/100], Loss: 1.5138\n",
      "Epoch [30/100], Loss: 1.0721\n",
      "Epoch [40/100], Loss: 1.0001\n",
      "Epoch [50/100], Loss: 0.9791\n",
      "Epoch [60/100], Loss: 0.9707\n",
      "Epoch [70/100], Loss: 0.9671\n",
      "Epoch [80/100], Loss: 0.9655\n",
      "Epoch [90/100], Loss: 0.9648\n",
      "Epoch [100/100], Loss: 0.9644\n",
      "Query: \"Amazing product\"\n",
      "Top-5 Reviews:\n",
      "1. \"Great product, fast delivery\" (Similarity: 0.5291)\n",
      "2. \"Awesome item, highly recommend\" (Similarity: 0.5288)\n",
      "3. \"Good value, decent product\" (Similarity: 0.5285)\n",
      "4. \"Poor quality, broke quickly\" (Similarity: 0.4981)\n",
      "5. \"Terrible service, very slow\" (Similarity: 0.4980)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations and neural network functionality.\n",
    "# Theory: PyTorch provides tensors with autograd for training the embedding model.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules to define the embedding model.\n",
    "# Theory: nn.Module enables custom layers for embedding generation.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Purpose: Import optimization algorithms like Adam for training.\n",
    "# Theory: Adam adapts learning rates, suitable for contrastive loss optimization.\n",
    "\n",
    "import numpy as np\n",
    "# Purpose: Import NumPy for similarity computation and data handling.\n",
    "# Theory: NumPy’s array operations are efficient for cosine similarity and ranking.\n",
    "\n",
    "from collections import Counter\n",
    "# Purpose: Import Counter for building a simple word-based vocabulary.\n",
    "# Theory: Counts word frequencies to create a vocabulary for tokenization.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix random seed for consistent data and model initialization.\n",
    "# Theory: Ensures reproducibility, aligning with previous problems (e.g., RMS Norm).\n",
    "\n",
    "# Synthetic review dataset\n",
    "reviews = [\n",
    "    \"Great product, fast delivery\",\n",
    "    \"Awesome item, highly recommend\",\n",
    "    \"Poor quality, broke quickly\",\n",
    "    \"Terrible service, very slow\",\n",
    "    \"Good value, decent product\",\n",
    "    # ... (95 more reviews for a total of 100)\n",
    "] + [\"Good product, quick shipping\"] * 95  # Simplified for brevity\n",
    "# Purpose: Define a synthetic dataset of 100 reviews.\n",
    "# Theory: Mimics a review dataset for LLMs, with varied sentiments for testing retrieval.\n",
    "\n",
    "# Positive pairs (indices of similar reviews)\n",
    "positive_pairs = [(0, 1), (0, 4), (2, 3)]  # e.g., (0,1) are both positive\n",
    "# Purpose: Define pairs of reviews with similar sentiment or topic.\n",
    "# Theory: Used to train the embedding model with contrastive loss.\n",
    "\n",
    "# Build simple word-based vocabulary\n",
    "words = Counter(' '.join(reviews).split())\n",
    "# Purpose: Count word frequencies across all reviews.\n",
    "# Theory: Creates a vocabulary for tokenization, similar to BPE’s initial step.\n",
    "\n",
    "vocab = {word: i + 1 for i, word in enumerate(words)}  # 0 reserved for padding\n",
    "# Purpose: Map words to unique IDs, starting from 1.\n",
    "# Theory: Assigns token IDs for input to the embedding model.\n",
    "\n",
    "# Tokenize reviews\n",
    "def tokenize(text, vocab):\n",
    "    # Purpose: Convert text to token IDs.\n",
    "    # Theory: Splits text into words and maps to vocabulary IDs.\n",
    "    \n",
    "    return [vocab.get(word, 0) for word in text.split()]\n",
    "    # Purpose: Map each word to its ID, using 0 for unknown words.\n",
    "    # Theory: Handles out-of-vocabulary words with padding ID.\n",
    "\n",
    "tokenized_reviews = [tokenize(review, vocab) for review in reviews]\n",
    "# Purpose: Tokenize all reviews into lists of token IDs.\n",
    "# Theory: Prepares reviews for embedding generation.\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_len = max(len(tokens) for tokens in tokenized_reviews)\n",
    "# Purpose: Find the maximum sequence length for padding.\n",
    "# Theory: Ensures consistent input shapes for batch processing.\n",
    "\n",
    "tokenized_reviews = [tokens + [0] * (max_len - len(tokens)) for tokens in tokenized_reviews]\n",
    "# Purpose: Pad sequences with zeros to match max_len.\n",
    "# Theory: Enables batch processing in PyTorch with fixed-size tensors.\n",
    "\n",
    "# Convert to tensor\n",
    "review_tensors = torch.tensor(tokenized_reviews, dtype=torch.long)\n",
    "# Purpose: Convert tokenized reviews to a PyTorch tensor.\n",
    "# Theory: Shape [100, max_len] for input to the embedding model.\n",
    "\n",
    "# Define embedding model\n",
    "class EmbeddingModel(nn.Module):\n",
    "    # Purpose: Define a neural network to generate review embeddings.\n",
    "    # Theory: Maps tokenized reviews to fixed-size embeddings using embedding and linear layers.\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=64):\n",
    "        # Purpose: Initialize the model with vocabulary size and embedding dimension.\n",
    "        # Theory: vocab_size is the number of unique tokens; embed_dim (64) is the embedding size.\n",
    "        \n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        # Purpose: Call parent nn.Module constructor.\n",
    "        # Theory: Registers parameters for autograd.\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Purpose: Define embedding layer for token IDs.\n",
    "        # Theory: Maps each token ID to a 64-dimensional vector.\n",
    "        \n",
    "        self.linear = nn.Linear(embed_dim, embed_dim)\n",
    "        # Purpose: Define linear layer to transform aggregated embeddings.\n",
    "        # Theory: Adds learnable transformation to capture semantic relationships.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Compute embeddings for input token IDs.\n",
    "        # Theory: Embeds tokens and aggregates to produce a review-level embedding.\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # Purpose: Convert token IDs to embeddings.\n",
    "        # Theory: Shape [batch_size, seq_len, embed_dim].\n",
    "        \n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        # Purpose: Create mask for non-padding tokens.\n",
    "        # Theory: Zeros out padding embeddings during aggregation.\n",
    "        \n",
    "        embedded = embedded * mask\n",
    "        # Purpose: Apply mask to ignore padding tokens.\n",
    "        # Theory: Ensures padding doesn’t affect the mean embedding.\n",
    "        \n",
    "        mean_embedded = embedded.sum(dim=1) / mask.sum(dim=1)\n",
    "        # Purpose: Compute mean embedding across non-padding tokens.\n",
    "        # Theory: Aggregates token embeddings to a single vector per review.\n",
    "        \n",
    "        return self.linear(mean_embedded)\n",
    "        # Purpose: Apply linear transformation to the aggregated embedding.\n",
    "        # Theory: Outputs final embedding, shape [batch_size, embed_dim].\n",
    "\n",
    "# Define contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    # Purpose: Define contrastive loss for training the embedding model.\n",
    "    # Theory: Encourages similar reviews to have close embeddings, dissimilar ones to be far apart.\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        # Purpose: Initialize loss with temperature parameter.\n",
    "        # Theory: Temperature (0.07) controls the softness of similarity scores.\n",
    "        \n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        # Purpose: Store temperature for use in forward pass.\n",
    "        # Theory: Scales cosine similarities for sharper distributions.\n",
    "    \n",
    "    def forward(self, embeddings, positive_pairs):\n",
    "        # Purpose: Compute contrastive loss for a batch of embeddings.\n",
    "        # Theory: Uses cosine similarity to compare positive and negative pairs.\n",
    "        \n",
    "        embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "        # Purpose: Normalize embeddings to unit length.\n",
    "        # Theory: Ensures cosine similarity is computed correctly (dot product of unit vectors).\n",
    "        \n",
    "        loss = 0.0\n",
    "        # Purpose: Initialize loss accumulator.\n",
    "        # Theory: Sums loss over positive pairs.\n",
    "        \n",
    "        for i, j in positive_pairs:\n",
    "            # Purpose: Iterate over positive pairs.\n",
    "            # Theory: Computes loss for each pair of similar reviews.\n",
    "            \n",
    "            sim_pos = torch.sum(embeddings[i] * embeddings[j]) / self.temperature\n",
    "            # Purpose: Compute similarity for positive pair.\n",
    "            # Theory: Dot product of normalized embeddings, scaled by temperature.\n",
    "            \n",
    "            sim_neg = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "            # Purpose: Compute similarities for all pairs (including negatives).\n",
    "            # Theory: Matrix multiplication gives all pairwise similarities.\n",
    "            \n",
    "            loss += -sim_pos + torch.logsumexp(sim_neg[i], dim=0)\n",
    "            # Purpose: Add contrastive loss for the positive pair.\n",
    "            # Theory: -log(exp(sim_pos) / sum(exp(sim_neg))) pushes positive pairs closer, negatives apart.\n",
    "        \n",
    "        return loss / len(positive_pairs)\n",
    "        # Purpose: Average loss over positive pairs.\n",
    "        # Theory: Normalizes loss to make it independent of pair count.\n",
    "\n",
    "# Define RAGSearch class\n",
    "class RAGSearch:\n",
    "    # Purpose: Define RAG system for embedding and retrieving reviews.\n",
    "    # Theory: Combines embedding generation and similarity search for retrieval.\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=64):\n",
    "        # Purpose: Initialize RAG system with embedding model.\n",
    "        # Theory: Sets up model and vocabulary for encoding and searching.\n",
    "        \n",
    "        self.model = EmbeddingModel(vocab_size, embed_dim)\n",
    "        # Purpose: Initialize embedding model.\n",
    "        # Theory: Prepares model for training and inference.\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        # Purpose: Store vocabulary for tokenization.\n",
    "        # Theory: Maps words to IDs for input processing.\n",
    "    \n",
    "    def train(self, reviews, positive_pairs, epochs=100):\n",
    "        # Purpose: Train the embedding model using contrastive loss.\n",
    "        # Theory: Optimizes embeddings to cluster similar reviews.\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # Purpose: Initialize Adam optimizer with learning rate 0.001.\n",
    "        # Theory: Adam adapts learning rates for efficient training.\n",
    "        \n",
    "        criterion = ContrastiveLoss()\n",
    "        # Purpose: Initialize contrastive loss.\n",
    "        # Theory: Used to train embeddings with positive and negative pairs.\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Purpose: Iterate over epochs for training.\n",
    "            # Theory: Updates model parameters to minimize loss.\n",
    "            \n",
    "            self.model.train()\n",
    "            # Purpose: Set model to training mode.\n",
    "            # Theory: Enables gradient computation and parameter updates.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Purpose: Reset gradients.\n",
    "            # Theory: Prevents gradient accumulation.\n",
    "            \n",
    "            embeddings = self.model(review_tensors)\n",
    "            # Purpose: Compute embeddings for all reviews.\n",
    "            # Theory: Shape [100, 64], representing each review in embedding space.\n",
    "            \n",
    "            loss = criterion(embeddings, positive_pairs)\n",
    "            # Purpose: Compute contrastive loss.\n",
    "            # Theory: Encourages similar embeddings for positive pairs.\n",
    "            \n",
    "            loss.backward()\n",
    "            # Purpose: Compute gradients.\n",
    "            # Theory: Backpropagates loss through the model.\n",
    "            \n",
    "            optimizer.step()\n",
    "            # Purpose: Update model parameters.\n",
    "            # Theory: Applies gradient-based updates to minimize loss.\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # Purpose: Print loss every 10 epochs.\n",
    "                # Theory: Monitors training progress.\n",
    "                \n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "                # Purpose: Display epoch and loss.\n",
    "                # Theory: loss.item() extracts scalar loss for readability.\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Purpose: Encode a text (query or review) into an embedding.\n",
    "        # Theory: Converts text to tokens, then to a fixed-size embedding.\n",
    "        \n",
    "        tokens = tokenize(text, self.vocab)\n",
    "        # Purpose: Tokenize input text.\n",
    "        # Theory: Converts text to token IDs using the vocabulary.\n",
    "        \n",
    "        tokens = tokens + [0] * (max_len - len(tokens))\n",
    "        # Purpose: Pad tokens to match max_len.\n",
    "        # Theory: Ensures consistent input shape for the model.\n",
    "        \n",
    "        tokens_tensor = torch.tensor([tokens], dtype=torch.long)\n",
    "        # Purpose: Convert tokens to tensor.\n",
    "        # Theory: Shape [1, max_len] for single text input.\n",
    "        \n",
    "        self.model.eval()\n",
    "        # Purpose: Set model to evaluation mode.\n",
    "        # Theory: Disables gradient computation for inference.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Purpose: Disable gradient tracking.\n",
    "            # Theory: Saves memory during inference.\n",
    "            \n",
    "            embedding = self.model(tokens_tensor)\n",
    "            # Purpose: Compute embedding for the text.\n",
    "            # Theory: Outputs [1, 64] embedding vector.\n",
    "            \n",
    "            return embedding.numpy()\n",
    "            # Purpose: Convert embedding to NumPy array.\n",
    "            # Theory: Facilitates similarity computation with NumPy.\n",
    "    \n",
    "    def search(self, query, reviews, top_k=5):\n",
    "        # Purpose: Retrieve top-k reviews similar to the query.\n",
    "        # Theory: Uses cosine similarity to rank reviews by embedding proximity.\n",
    "        \n",
    "        query_embedding = self.encode(query)\n",
    "        # Purpose: Encode the query into an embedding.\n",
    "        # Theory: Shape [1, 64] for similarity comparison.\n",
    "        \n",
    "        review_embeddings = np.vstack([self.encode(review) for review in reviews])\n",
    "        # Purpose: Encode all reviews into embeddings.\n",
    "        # Theory: Shape [100, 64] for batch similarity computation.\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "        review_embeddings = review_embeddings / np.linalg.norm(review_embeddings, axis=1, keepdims=True)\n",
    "        # Purpose: Normalize embeddings to unit length.\n",
    "        # Theory: Ensures cosine similarity is computed correctly.\n",
    "        \n",
    "        similarities = np.dot(review_embeddings, query_embedding.T).flatten()\n",
    "        # Purpose: Compute cosine similarities between query and reviews.\n",
    "        # Theory: Dot product of normalized vectors gives cosine similarity.\n",
    "        \n",
    "        top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        # Purpose: Get indices of top-k most similar reviews.\n",
    "        # Theory: Sorts similarities in descending order to select top-k.\n",
    "        \n",
    "        return [(reviews[i], similarities[i]) for i in top_k_indices]\n",
    "        # Purpose: Return top-k reviews and their similarity scores.\n",
    "        # Theory: Provides ranked list for evaluation.\n",
    "\n",
    "# Test RAGSearch\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Test the RAGSearch implementation.\n",
    "    # Theory: Demonstrates training, encoding, and retrieval on the review dataset.\n",
    "    \n",
    "    rag = RAGSearch(len(vocab) + 1)  # +1 for padding token\n",
    "    # Purpose: Initialize RAG system with vocabulary size.\n",
    "    # Theory: Sets up embedding model for training and inference.\n",
    "    \n",
    "    rag.train(reviews, positive_pairs, epochs=100)\n",
    "    # Purpose: Train the embedding model.\n",
    "    # Theory: Optimizes embeddings using contrastive loss.\n",
    "    \n",
    "    query = \"Amazing product\"\n",
    "    # Purpose: Define a test query.\n",
    "    # Theory: Tests retrieval of reviews with similar sentiment.\n",
    "    \n",
    "    top_k_reviews = rag.search(query, reviews, top_k=5)\n",
    "    # Purpose: Retrieve top-5 reviews for the query.\n",
    "    # Theory: Uses cosine similarity to find relevant reviews.\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    print(\"Top-5 Reviews:\")\n",
    "    for i, (review, sim) in enumerate(top_k_reviews):\n",
    "        # Purpose: Print top-k reviews with similarity scores.\n",
    "        # Theory: Shows retrieval quality based on embedding similarity.\n",
    "        \n",
    "        print(f\"{i + 1}. \\\"{review}\\\" (Similarity: {sim:.4f})\")\n",
    "        # Purpose: Display review and its similarity score.\n",
    "        # Theory: High scores (~0.9) indicate successful retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4c5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
