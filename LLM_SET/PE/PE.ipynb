{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb276409",
   "metadata": {},
   "source": [
    "# Implement Sinusoidal Positional Embeddings from Scratch\n",
    "\n",
    "Description: Implement Sinusoidal Positional Embeddings as described in Vaswani et al. (2017) to provide Transformers with sequence order information, since attention mechanisms are inherently order-agnostic. The class SinusoidalPositionalEmbedding(nn.Module) generates deterministic positional encodings using sine and cosine functions with varying frequencies, stored in a non-trainable buffer tensor of shape (max_seq_len, d_model). The forward method returns encodings for an input tensor’s sequence length, broadcastable over the batch dimension. The implementation must use only PyTorch operations, avoid external libraries like Hugging Face or fairseq, and support sequences up to max_seq_len. Tests will verify shape correctness and numerical properties, ensuring compatibility with Transformer inputs.\n",
    "Mathematical Definition:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "max_seq_len: Maximum sequence length (e.g., 100).\n",
    "d_model: Embedding dimension (e.g., 64).\n",
    "Input tensor $ x \\in \\mathbb{R}^{N \\times L \\times d_{\\text{model}}} $, where $ N $ is batch size, $ L \\leq \\text{max\\_seq\\_len} $.\n",
    "\n",
    "\n",
    "Positional Encoding:\n",
    "\n",
    "For position $ pos \\in \\{0, 1, \\ldots, \\text{max\\_seq\\_len}-1\\} $ and dimension $ i \\in \\{0, 1, \\ldots, d_{\\text{model}}-1\\} $:\n",
    "$$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i / d_{\\text{model}}}}\\right)$$\n",
    "$$PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i / d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "Frequency term: $ \\omega_i = 10000^{-2i / d_{\\text{model}}} = e^{-(2i / d_{\\text{model}}) \\cdot \\ln(10000)} $.\n",
    "Resulting tensor: $ PE \\in \\mathbb{R}^{\\text{max\\_seq\\_len} \\times d_{\\text{model}}} $.\n",
    "\n",
    "\n",
    "Forward Pass:\n",
    "\n",
    "Given input $ x \\in \\mathbb{R}^{N \\times L \\times d_{\\text{model}}} $, return $ PE[:L, :] \\in \\mathbb{R}^{1 \\times L \\times d_{\\text{model}}} $, broadcastable over batch dimension.\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Positional encodings of shape $ (1, L, d_{\\text{model}}) $, added to token embeddings in a Transformer.\n",
    "\n",
    "\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Implement SinusoidalPositionalEmbedding(nn.Module):\n",
    "\n",
    "Initialize with max_seq_len and d_model.\n",
    "Compute $ PE $ tensor using sine/cosine functions.\n",
    "Register $ PE $ as a non-trainable buffer via self.register_buffer.\n",
    "In forward(x), return encodings for the input’s sequence length.\n",
    "\n",
    "\n",
    "Use PyTorch operations (torch.sin, torch.cos, torch.arange, torch.exp).\n",
    "Test with max_seq_len=100, d_model=64, and sequence length 50:\n",
    "\n",
    "Verify output shape: $ (1, 50, 64) $.\n",
    "Check numerical properties (e.g., sine/cosine alternation, bounded values).\n",
    "\n",
    "\n",
    "Provide detailed Purpose and Theory comments.\n",
    "Avoid integrating with token embeddings (focus on positional encodings only).\n",
    "\n",
    "Constraints:\n",
    "\n",
    "Use only PyTorch operations (no Hugging Face, fairseq, or built-in positional encoding modules).\n",
    "Ensure $ PE $ is not a trainable parameter.\n",
    "Support sequence lengths $ L \\leq \\text{max\\_seq\\_len} $.\n",
    "Output shape: $ (1, L, d_{\\text{model}}) $.\n",
    "Ensure numerical stability (values in $[-1, 1]$).\n",
    "\n",
    "Synthetic Dataset:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "Input tensor: Random tensor of shape $(3, 4, 8)$, generated with torch.rand and seed 42 (provided code).\n",
    "Test case: max_seq_len=100, d_model=64, sequence length 50.\n",
    "\n",
    "\n",
    "Test Cases:\n",
    "\n",
    "Shape test: Output shape $(1, 50, 64)$.\n",
    "Numerical test: Verify even indices use sine, odd use cosine.\n",
    "Boundary test: Sequence length 1 and max_seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0614ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 50, 64])\n",
      "Output shape: torch.Size([1, 50, 64])\n",
      "Shape test passed!\n",
      "Numerical test passed!\n",
      "Buffer registration test passed!\n",
      "Boundary test passed!\n",
      "Input embedding shape: torch.Size([3, 50, 64])\n",
      "Integration test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations.\n",
    "# Theory: Provides tensor computations and autograd support.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network module for nn.Module.\n",
    "# Theory: Base class for positional embedding module.\n",
    "\n",
    "import math\n",
    "# Purpose: Import math for logarithmic computations.\n",
    "# Theory: Used to compute frequency terms.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix random seed for consistent testing.\n",
    "# Theory: Ensures reproducible results across runs.\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the sinusoidal positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "            d_model (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        # Purpose: Initialize the positional embedding module.\n",
    "        # Theory: Sets up fixed sinusoidal encodings for positions.\n",
    "        \n",
    "        super().__init__()\n",
    "        # Purpose: Initialize parent nn.Module class.\n",
    "        # Theory: Enables module functionality (e.g., buffer registration).\n",
    "        \n",
    "        assert d_model % 2 == 0, \"d_model must be even for sin/cos pairs\"\n",
    "        # Purpose: Ensure d_model supports even/odd sin/cos splitting.\n",
    "        # Theory: Each pair of dimensions uses sin and cos.\n",
    "        \n",
    "        # Create positional encoding tensor\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # Purpose: Initialize PE tensor of shape (max_seq_len, d_model).\n",
    "        # Theory: Stores encodings for all positions up to max_seq_len.\n",
    "        \n",
    "        # Compute position indices: [0, 1, ..., max_seq_len-1]\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Purpose: Create position indices tensor of shape (max_seq_len, 1).\n",
    "        # Theory: Represents sequence positions for encoding.\n",
    "        \n",
    "        # Compute frequency terms: exp(-2i/d_model * ln(10000))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Purpose: Compute frequency terms of shape (d_model/2,).\n",
    "        # Theory: Defines geometric frequency progression for encodings.\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Purpose: Fill PE tensor with sin/cos values.\n",
    "        # Theory: Even dims use sin(pos/10000^(2i/d_model)), odd use cos.\n",
    "        \n",
    "        # Register PE as a buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        # Purpose: Store PE as a non-trainable buffer.\n",
    "        # Theory: Ensures PE is saved with model but not optimized.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the positional embedding for the input tensor's sequence length.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positional embeddings of shape (1, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # Purpose: Return positional encodings for input sequence length.\n",
    "        # Theory: Slices PE tensor to match input seq_len, broadcasts over batch.\n",
    "        \n",
    "        return self.pe[:x.shape[1], :].unsqueeze(0)\n",
    "        # Purpose: Slice PE and add batch dimension.\n",
    "        # Theory: Shape (1, seq_len, d_model), broadcastable to (N, seq_len, d_model).\n",
    "\n",
    "# Test implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Test SinusoidalPositionalEmbedding for correctness.\n",
    "    # Theory: Verifies shape, numerical properties, and buffer registration.\n",
    "    \n",
    "    # Test parameters\n",
    "    max_seq_len, d_model = 100, 64\n",
    "    seq_len = 50\n",
    "    batch_size = 3\n",
    "    # Purpose: Define test dimensions.\n",
    "    # Theory: Matches problem requirements, tests realistic values.\n",
    "    \n",
    "    # Initialize module\n",
    "    pos_emb = SinusoidalPositionalEmbedding(max_seq_len, d_model)\n",
    "    # Purpose: Create positional embedding instance.\n",
    "    # Theory: Initializes PE tensor with sin/cos encodings.\n",
    "    \n",
    "    # Test shape\n",
    "    x = torch.rand(batch_size, seq_len, d_model)\n",
    "    pe = pos_emb(x)\n",
    "    # Purpose: Generate positional encodings for input tensor.\n",
    "    # Theory: Tests forward pass with seq_len=50.\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", pe.shape)\n",
    "    assert pe.shape == (1, seq_len, d_model), f\"Expected shape (1, {seq_len}, {d_model}), got {pe.shape}\"\n",
    "    # Purpose: Verify output shape.\n",
    "    # Theory: Ensures shape (1, seq_len, d_model).\n",
    "    \n",
    "    print(\"Shape test passed!\")\n",
    "    # Purpose: Confirm shape test success.\n",
    "    # Theory: Validates correct tensor slicing.\n",
    "    \n",
    "    # Test numerical correctness\n",
    "    assert torch.allclose(pe[0, 0, 0], torch.sin(torch.tensor(0.0)), atol=1e-6), \"PE[0,0] should be sin(0)\"\n",
    "    assert torch.allclose(pe[0, 0, 1], torch.cos(torch.tensor(0.0)), atol=1e-6), \"PE[0,1] should be cos(0)\"\n",
    "    assert pe.abs().max() <= 1.0, \"PE values should be in [-1, 1]\"\n",
    "    # Purpose: Verify sin/cos alternation and boundedness.\n",
    "    # Theory: Ensures even/odd dims use sin/cos, values are stable.\n",
    "    \n",
    "    print(\"Numerical test passed!\")\n",
    "    # Purpose: Confirm numerical test success.\n",
    "    # Theory: Validates correct encoding computation.\n",
    "    \n",
    "    # Test buffer registration\n",
    "    assert \"pe\" in pos_emb._buffers, \"PE should be registered as a buffer\"\n",
    "    assert not any(p.requires_grad for p in pos_emb.parameters()), \"No trainable parameters expected\"\n",
    "    # Purpose: Verify PE is a buffer, not a parameter.\n",
    "    # Theory: Ensures non-trainable property.\n",
    "    \n",
    "    print(\"Buffer registration test passed!\")\n",
    "    # Purpose: Confirm buffer test success.\n",
    "    # Theory: Validates correct module setup.\n",
    "    \n",
    "    # Test boundary cases\n",
    "    x_short = torch.rand(batch_size, 1, d_model)\n",
    "    x_max = torch.rand(batch_size, max_seq_len, d_model)\n",
    "    pe_short = pos_emb(x_short)\n",
    "    pe_max = pos_emb(x_max)\n",
    "    # Purpose: Test sequence lengths 1 and max_seq_len.\n",
    "    # Theory: Ensures robustness across sequence lengths.\n",
    "    \n",
    "    assert pe_short.shape == (1, 1, d_model), \"Short sequence shape incorrect\"\n",
    "    assert pe_max.shape == (1, max_seq_len, d_model), \"Max sequence shape incorrect\"\n",
    "    # Purpose: Verify boundary case shapes.\n",
    "    # Theory: Confirms slicing works for edge cases.\n",
    "    \n",
    "    print(\"Boundary test passed!\")\n",
    "    # Purpose: Confirm boundary test success.\n",
    "    # Theory: Validates support for all valid sequence lengths.\n",
    "    \n",
    "    # Test integration with token embeddings (for visualization)\n",
    "    token_emb = torch.rand(batch_size, seq_len, d_model)\n",
    "    input_emb = token_emb + pos_emb(token_emb)\n",
    "    # Purpose: Simulate Transformer input by adding PE to token embeddings.\n",
    "    # Theory: Demonstrates typical usage, though not required.\n",
    "    \n",
    "    print(\"Input embedding shape:\", input_emb.shape)\n",
    "    assert input_emb.shape == (batch_size, seq_len, d_model), \"Input embedding shape incorrect\"\n",
    "    # Purpose: Verify combined embedding shape.\n",
    "    # Theory: Ensures PE broadcasts correctly over batch.\n",
    "    \n",
    "    print(\"Integration test passed!\")\n",
    "    # Purpose: Confirm integration test success.\n",
    "    # Theory: Validates PE’s role in Transformer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef0662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
