{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5e1ca1",
   "metadata": {},
   "source": [
    "# RMS Normalization Implementation\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Title**: Implement RMS Normalization for Large Language Models\n",
    "\n",
    "**Description**: You are tasked with implementing **Root Mean Square (RMS) Normalization**, a normalization technique used in Large Language Models (LLMs) like LLaMA, to stabilize training by scaling activations based on their root mean square value. RMSNorm is a simpler alternative to LayerNorm, omitting mean subtraction and bias terms. Your implementation should be integrated into a simple transformer-like block to demonstrate its effect on stabilizing activations. Use PyTorch to define the RMSNorm class and a small model, and train it on a synthetic dataset to verify that normalization maintains stable output scales while learning a target transformation.\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "For an input vector x ∈ R^d (dimension d), RMSNorm computes:\n",
    "\n",
    "```\n",
    "RMS(x) = √((1/d) * ∑_{i=1}^d x_i^2 + ε)\n",
    "```\n",
    "\n",
    "```\n",
    "x̂_i = (x_i / RMS(x)) * g_i\n",
    "```\n",
    "\n",
    "where:\n",
    "- ε: Small constant (e.g., 10^-5) for numerical stability\n",
    "- g_i: Learnable scale parameter (vector of size d)\n",
    "- x̂_i: Normalized output\n",
    "\n",
    "For a batch X ∈ R^{n×d} (n samples), compute RMS per sample:\n",
    "\n",
    "```\n",
    "RMS(X_j) = √((1/d) * ∑_{i=1}^d X_{j,i}^2 + ε)\n",
    "```\n",
    "\n",
    "```\n",
    "X̂_{j,i} = (X_{j,i} / RMS(X_j)) * g_i\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Implement an `RMSNorm` class inheriting from `torch.nn.Module`\n",
    "- Define `forward` to compute RMS normalization with learnable scale parameters\n",
    "- Integrate RMSNorm into a simple model (e.g., RMSNorm followed by a linear layer)\n",
    "- Use a synthetic dataset of random embeddings (100 samples, 64 dimensions)\n",
    "- Train the model with Mean Squared Error (MSE) loss and Adam optimizer\n",
    "- Evaluate the model's output stability (e.g., variance of normalized outputs) and loss convergence\n",
    "- Provide detailed **Purpose** and **Theory** comments for each line of code\n",
    "\n",
    "## Constraints\n",
    "\n",
    "- Use only PyTorch for tensor operations and model definition (no scikit-learn or other ML libraries)\n",
    "- Handle batch inputs (X ∈ R^{n×d})\n",
    "- Ensure numerical stability with ε = 10^-5\n",
    "- Ensure compatibility with PyTorch's autograd for training\n",
    "- Use a learning rate of 0.01 and train for 1000 epochs\n",
    "\n",
    "## Synthetic Dataset\n",
    "\n",
    "- **Input**: X ∈ R^{100×64}, random embeddings drawn from a normal distribution N(0,1)\n",
    "- **Target**: y = 0.5 · X + noise, where noise is N(0,0.1), shape [100,64]\n",
    "- **Test Data**: 2 samples, shape [2,64], to verify model generalization\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "- **Loss**: Decreases from ~0.1 to ~0.01 over 1000 epochs, indicating convergence\n",
    "- **Normalized outputs**: Variance close to 1 (due to normalization), scaled by learnable parameters\n",
    "- **Test predictions**: Approximate y = 0.5 · X_test, with stable magnitudes due to RMSNorm\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### RMSNorm Class Structure\n",
    "\n",
    "```python\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Purpose: Initialize RMS Normalization layer\n",
    "        Theory: RMSNorm normalizes by RMS value instead of mean/variance\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize learnable scale parameters\n",
    "        # Initialize epsilon for numerical stability\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Purpose: Apply RMS normalization to input tensor\n",
    "        Theory: Normalize by RMS and apply learnable scaling\n",
    "        \"\"\"\n",
    "        # Compute RMS per sample\n",
    "        # Apply normalization and scaling\n",
    "        # Return normalized output\n",
    "```\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "```python\n",
    "class SimpleRMSModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Purpose: Simple model with RMSNorm + Linear layer\n",
    "        Theory: Demonstrates RMSNorm's stabilization effect\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # RMSNorm layer\n",
    "        # Linear transformation layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply RMSNorm\n",
    "        # Apply linear transformation\n",
    "        # Return output\n",
    "```\n",
    "\n",
    "### Training Pipeline\n",
    "\n",
    "1. **Data Generation**: Create synthetic dataset with target transformation\n",
    "2. **Model Initialization**: Initialize RMSModel with appropriate dimensions\n",
    "3. **Training Loop**: \n",
    "   - Forward pass through model\n",
    "   - Compute MSE loss\n",
    "   - Backpropagation and optimization\n",
    "   - Track loss convergence\n",
    "4. **Evaluation**: Assess normalization stability and test predictions\n",
    "\n",
    "## Key Implementation Details\n",
    "\n",
    "### Numerical Stability\n",
    "- Add epsilon (ε = 1e-5) to prevent division by zero\n",
    "- Use stable computation of RMS to avoid numerical issues\n",
    "\n",
    "### Batch Processing\n",
    "- Compute RMS per sample (across feature dimension)\n",
    "- Maintain batch dimension throughout computation\n",
    "\n",
    "### Learnable Parameters\n",
    "- Initialize scale parameters (g) appropriately\n",
    "- Ensure parameters are registered for gradient computation\n",
    "\n",
    "### Gradient Flow\n",
    "- Maintain differentiability for backpropagation\n",
    "- Verify gradients flow through normalization layer\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **MSE Loss**: Training loss convergence over epochs\n",
    "- **Variance Analysis**: Variance of normalized outputs (should be ~1)\n",
    "- **Test Predictions**: Model generalization on unseen data\n",
    "- **Stability Check**: Output magnitude consistency\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "```python\n",
    "# 1. RMSNorm implementation\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    # Implementation with detailed comments\n",
    "\n",
    "# 2. Simple model with RMSNorm\n",
    "class SimpleRMSModel(torch.nn.Module):\n",
    "    # Model architecture\n",
    "\n",
    "# 3. Data generation\n",
    "def generate_synthetic_data():\n",
    "    # Create training and test datasets\n",
    "\n",
    "# 4. Training function\n",
    "def train_model():\n",
    "    # Training loop with loss tracking\n",
    "\n",
    "# 5. Evaluation function\n",
    "def evaluate_model():\n",
    "    # Compute evaluation metrics\n",
    "\n",
    "# 6. Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete pipeline\n",
    "```\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "### Training Convergence\n",
    "- Initial loss: ~0.1\n",
    "- Final loss: ~0.01\n",
    "- Smooth convergence over 1000 epochs\n",
    "\n",
    "### Normalization Effect\n",
    "- Normalized output variance ≈ 1.0\n",
    "- Stable activation magnitudes\n",
    "- Consistent scaling across batches\n",
    "\n",
    "### Test Performance\n",
    "- Predictions approximate y = 0.5 · X_test\n",
    "- Stable output magnitudes\n",
    "- Good generalization to unseen data\n",
    "\n",
    "## Comparison with LayerNorm\n",
    "\n",
    "| Feature | RMSNorm | LayerNorm |\n",
    "|---------|---------|-----------|\n",
    "| Mean Subtraction | No | Yes |\n",
    "| Bias Term | No | Yes |\n",
    "| Computational Cost | Lower | Higher |\n",
    "| Parameters | d (scale only) | 2d (scale + bias) |\n",
    "| Stability | Good | Excellent |\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "# Create model\n",
    "model = SimpleRMSModel(input_dim=64, output_dim=64)\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_synthetic_data()\n",
    "\n",
    "# Train model\n",
    "train_model(model, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, X_test, y_test)\n",
    "```\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. Complete `RMSNorm` class implementation\n",
    "2. `SimpleRMSModel` integration\n",
    "3. Synthetic dataset generation\n",
    "4. Training pipeline with loss tracking\n",
    "5. Evaluation metrics and analysis\n",
    "6. Detailed code comments explaining purpose and theory\n",
    "7. Results validation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Purpose: Import PyTorch for tensor operations and neural network functionality.\n",
    "# Theory: PyTorch provides tensors with GPU support and autograd for automatic differentiation, essential for RMSNorm and training.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules to define RMSNorm and model classes.\n",
    "# Theory: nn.Module enables custom layers like RMSNorm to integrate with PyTorch’s autograd and parameter management.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Purpose: Import optimization algorithms like Adam for updating model parameters.\n",
    "# Theory: Adam uses adaptive learning rates (momentum and squared gradients), effective for training with normalization layers.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix the random seed to ensure consistent data generation and model initialization.\n",
    "# Theory: Reproducibility aligns with previous TorchLeet problems (e.g., KL Divergence, DNN), using seed 42 for consistency.\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples, d_model = 100, 64\n",
    "# Purpose: Define dataset size (100 samples) and embedding dimension (64).\n",
    "# Theory: Simulates transformer input embeddings (e.g., hidden states in LLMs). 64 is a typical small hidden size for testing.\n",
    "\n",
    "X = torch.randn(n_samples, d_model)\n",
    "# Purpose: Generate random input embeddings, shape [100, 64].\n",
    "# Theory: Normal distribution (mean 0, std 1) mimics activations in transformer layers, suitable for testing normalization.\n",
    "\n",
    "y = X * 0.5 + torch.randn(n_samples, d_model) * 0.1\n",
    "# Purpose: Generate target outputs as a scaled version of X with Gaussian noise, shape [100, 64].\n",
    "# Theory: Simulates a regression task where the model learns y = 0.5 * X + noise. Noise (std 0.1) adds realism.\n",
    "\n",
    "# Define RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    # Purpose: Define RMSNorm layer for normalizing activations in LLMs.\n",
    "    # Theory: Normalizes inputs by their root mean square, scaling with learnable parameters, to stabilize training.\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        # Purpose: Initialize RMSNorm with embedding dimension and epsilon.\n",
    "        # Theory: d_model is the feature dimension (e.g., 64); eps prevents division by zero in RMS computation.\n",
    "        \n",
    "        super(RMSNorm, self).__init__()\n",
    "        # Purpose: Call parent nn.Module constructor to set up the module.\n",
    "        # Theory: Registers parameters and enables autograd integration for training.\n",
    "        \n",
    "        self.eps = eps\n",
    "        # Purpose: Store epsilon as an instance variable for numerical stability.\n",
    "        # Theory: Small constant (1e-5) ensures the RMS denominator is non-zero, preventing division errors.\n",
    "        \n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        # Purpose: Initialize learnable scale parameters as a vector of ones, shape [d_model].\n",
    "        # Theory: Scales normalized outputs, allowing the model to learn optimal magnitudes. Initialized to 1 for stable starting point.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Compute RMSNorm for input tensor x.\n",
    "        # Theory: Normalizes x by dividing by RMS(x) = sqrt(mean(x^2) + ε), then scales with learnable scale. x shape: [batch_size, d_model].\n",
    "        \n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        # Purpose: Compute RMS for each sample in the batch.\n",
    "        # Theory: mean(x^2, dim=-1) averages squared elements along the feature dimension; sqrt adds ε for stability. Shape: [batch_size, 1].\n",
    "        \n",
    "        x_norm = x / rms\n",
    "        # Purpose: Normalize input by dividing by RMS.\n",
    "        # Theory: Scales each element to have unit RMS, reducing covariate shift. Output shape: [batch_size, d_model].\n",
    "        \n",
    "        return x_norm * self.scale\n",
    "        # Purpose: Apply learnable scale to normalized output.\n",
    "        # Theory: Element-wise multiplication with scale parameter allows adaptive magnitudes. Output shape: [batch_size, d_model].\n",
    "\n",
    "# Define a simple model with RMSNorm\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    # Purpose: Define a transformer-like block with RMSNorm and a linear layer.\n",
    "    # Theory: Mimics a transformer’s feed-forward block, testing RMSNorm’s stabilization in a realistic setting.\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super(SimpleTransformerBlock, self).__init__()\n",
    "        # Purpose: Initialize parent nn.Module class.\n",
    "        # Theory: Ensures proper parameter registration for autograd and model training.\n",
    "        \n",
    "        self.norm = RMSNorm(d_model)\n",
    "        # Purpose: Initialize RMSNorm layer for input normalization.\n",
    "        # Theory: Normalizes inputs before linear transformation, as in LLMs like LLaMA.\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        # Purpose: Initialize a linear layer to transform normalized inputs.\n",
    "        # Theory: Applies z = Wx + b, where W is [d_model, d_model], b is [d_model], simulating a feed-forward layer.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Define forward pass through normalization and linear layers.\n",
    "        # Theory: Normalizes inputs to stabilize training, then applies a linear transformation.\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        # Purpose: Apply RMSNorm to input tensor.\n",
    "        # Theory: Stabilizes activations, ensuring consistent scales before further processing.\n",
    "        \n",
    "        return self.linear(x)\n",
    "        # Purpose: Apply linear transformation to normalized input.\n",
    "        # Theory: Outputs transformed embeddings, shape [batch_size, d_model], for downstream tasks.\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleTransformerBlock(d_model)\n",
    "# Purpose: Create an instance of the transformer block.\n",
    "# Theory: Initializes weights (Xavier initialization) and scale parameters (ones), tracked by autograd.\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Purpose: Define Mean Squared Error loss for regression.\n",
    "# Theory: MSE = (1/n) * sum((y_pred - y_true)^2), suitable for regression, consistent with DNN regression (Day 4).\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Purpose: Initialize Adam optimizer with learning rate 0.01.\n",
    "# Theory: Adam adapts learning rates using momentum (β1=0.9, β2=0.999), effective for models with normalization.\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "# Purpose: Set the number of training iterations to 1000 epochs.\n",
    "# Theory: Sufficient epochs ensure convergence, aligning with previous TorchLeet problems (e.g., KL Divergence).\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Purpose: Iterate over the dataset for training.\n",
    "    # Theory: Each epoch updates parameters to minimize loss, testing RMSNorm’s effect on stability.\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    # Purpose: Compute model predictions by passing input through the model.\n",
    "    # Theory: X [100, 64] produces predictions [100, 64] via RMSNorm and linear layer.\n",
    "    \n",
    "    loss = criterion(predictions, y)\n",
    "    # Purpose: Calculate MSE loss between predictions and targets.\n",
    "    # Theory: Computes scalar loss for optimization, measuring prediction accuracy.\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    # Purpose: Reset gradients of all parameters to zero.\n",
    "    # Theory: Prevents gradient accumulation from previous iterations, ensuring correct updates.\n",
    "    \n",
    "    loss.backward()\n",
    "    # Purpose: Compute gradients of the loss with respect to model parameters.\n",
    "    # Theory: Autograd backpropagates through linear layer, RMSNorm, and scale parameters.\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Purpose: Update model parameters using computed gradients.\n",
    "    # Theory: Adam applies adaptive updates to minimize loss, leveraging momentum.\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Purpose: Print training progress to monitor convergence.\n",
    "        # Theory: Loss monitoring helps detect issues like instability or poor learning rates.\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        # Purpose: Display epoch number and loss value.\n",
    "        # Theory: loss.item() extracts the scalar loss for readable output.\n",
    "\n",
    "# Evaluate normalized output variance\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for evaluation.\n",
    "    # Theory: Saves memory and computation during inference.\n",
    "    \n",
    "    normalized_output = model.norm(X)\n",
    "    # Purpose: Compute normalized outputs before the linear layer.\n",
    "    # Theory: Tests RMSNorm’s effect on stabilizing output variance.\n",
    "    \n",
    "    variance = torch.var(normalized_output, dim=-1).mean().item()\n",
    "    # Purpose: Calculate the mean variance of normalized outputs.\n",
    "    # Theory: Variance close to 1 indicates proper normalization (before scaling).\n",
    "    \n",
    "    print(f\"Normalized Output Variance (before scaling): {variance:.4f}\")\n",
    "    # Purpose: Print the variance to verify normalization.\n",
    "    # Theory: RMSNorm should produce outputs with unit RMS, adjusted by scale parameters.\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.randn(2, d_model)\n",
    "# Purpose: Generate test inputs, shape [2, 64].\n",
    "# Theory: Tests model generalization on new random embeddings.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for test inference.\n",
    "    # Theory: Ensures efficient evaluation without gradient computation.\n",
    "    \n",
    "    predictions = model(X_test)\n",
    "    # Purpose: Compute predictions for test inputs.\n",
    "    # Theory: Outputs [2, 64], approximating y = 0.5 * X_test + noise, with stable scales.\n",
    "    \n",
    "    print(f\"Test Predictions (first 5 dims): {predictions[:, :5].tolist()}\")\n",
    "    # Purpose: Print first 5 dimensions of test predictions for readability.\n",
    "    # Theory: Shows model output, expected to align with target transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d425e0d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
