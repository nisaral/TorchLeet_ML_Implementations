{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a382b190",
   "metadata": {},
   "source": [
    "# Implement Attention from Scratch\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement a Scaled Dot-Product Attention mechanism from scratch using PyTorch. Your mission (should you choose to accept it) is to replicate what PyTorch's built-in scaled_dot_product_attention does — manually. This core component is essential in Transformer architectures and helps models focus on relevant parts of a sequence. You'll test your implementation against PyTorch's native one to ensure you nailed it.\n",
    "\n",
    "## Requirements\n",
    "Define the Function:\n",
    "\n",
    "Create a function scaled_dot_product_attention(q, k, v, mask=None) that:\n",
    "Computes attention scores via the dot product of query and key vectors.\n",
    "Scales the scores using the square root of the key dimension.\n",
    "Applies an optional mask to the scores.\n",
    "Applies softmax to convert scores into attention weights.\n",
    "Uses these weights to compute a weighted sum of values (V).\n",
    "Test Your Work:\n",
    "\n",
    "Use sample tensors for query (Q), key (K), and value (V).\n",
    "Compare the result of your custom implementation with PyTorch's F.scaled_dot_product_attention using an assert to check numerical accuracy.\n",
    "Constraints\n",
    "Do NOT use F.scaled_dot_product_attention inside your custom function — that defeats the whole point.\n",
    "Your implementation must handle batch dimensions correctly.\n",
    "Support optional masking for future tokens or padding.\n",
    "Use only PyTorch ops — no cheating with external attention libs.\n",
    "Hint Use `torch.matmul()` to compute dot products and `F.softmax()` for the final attention weights. The mask (if used) should be applied **before** the softmax using `masked_fill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b665c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Output: tensor([[[ 0.0377, -0.3133,  0.8707],\n",
      "         [ 0.4541, -0.3508,  0.8461],\n",
      "         [ 0.3709, -0.2885,  0.8044]]])\n",
      "PyTorch Output: tensor([[[ 0.0377, -0.3133,  0.8707],\n",
      "         [ 0.4541, -0.3508,  0.8461],\n",
      "         [ 0.3709, -0.2885,  0.8044]]])\n",
      "Test without mask passed!\n",
      "Custom Output (with mask): tensor([[[-0.7658, -0.7506,  1.3525],\n",
      "         [ 0.4709, -0.3905,  0.8777],\n",
      "         [ 0.3709, -0.2885,  0.8044]]])\n",
      "PyTorch Output (with mask): tensor([[[ 0.2526, -0.1964,  0.7419],\n",
      "         [ 0.4312, -0.2969,  0.8033],\n",
      "         [ 0.3709, -0.2885,  0.8044]]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Masked outputs do not match!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 119\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch Output (with mask):\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_pytorch)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Purpose: Print masked outputs.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Theory: Verifies masking correctness.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(output_custom, output_pytorch, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-08\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-05\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked outputs do not match!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Purpose: Verify numerical equivalence for masked case.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Theory: Ensures masking is applied correctly.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest with causal mask passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Masked outputs do not match!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations.\n",
    "# Theory: Provides tensor computations with autograd support for attention mechanism.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# Purpose: Import functional module for softmax operation.\n",
    "# Theory: F.softmax is used to compute attention weights.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix random seed for consistent input tensors.\n",
    "# Theory: Ensures reproducible results, aligning with previous problems (e.g., RMS Norm).\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor of shape (..., seq_len_q, d_k)\n",
    "        k: Key tensor of shape (..., seq_len_k, d_k)\n",
    "        v: Value tensor of shape (..., seq_len_k, d_v)\n",
    "        mask: Optional mask tensor of shape (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output tensor of shape (..., seq_len_q, d_v)\n",
    "        attention_weights: Attention weights tensor of shape (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # Purpose: Define function to compute scaled dot-product attention.\n",
    "    # Theory: Implements the core attention mechanism from \"Attention is All You Need\".\n",
    "    \n",
    "    d_k = q.size(-1)\n",
    "    # Purpose: Get key dimension (d_k) from query tensor.\n",
    "    # Theory: d_k is used for scaling factor (sqrt(d_k)) to normalize attention scores.\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "    # Purpose: Compute dot product of query and key: Q * K^T.\n",
    "    # Theory: Produces raw attention scores, shape (..., seq_len_q, seq_len_k).\n",
    "    \n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    # Purpose: Scale scores by sqrt(d_k).\n",
    "    # Theory: Prevents large dot products in high dimensions, stabilizing gradients.\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Purpose: Check if a mask is provided.\n",
    "        # Theory: Mask handles causal attention (future tokens) or padding.\n",
    "        \n",
    "        scores = scores.masked_fill(mask == 1, -1e9)\n",
    "        # Purpose: Apply mask by setting masked positions to a large negative value.\n",
    "        # Theory: Ensures masked positions have near-zero weights after softmax.\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    # Purpose: Apply softmax to scores to get attention weights.\n",
    "    # Theory: Normalizes scores to probabilities, summing to 1 over the key dimension.\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    # Purpose: Compute weighted sum of values: A * V.\n",
    "    # Theory: Produces final output, shape (..., seq_len_q, d_v), combining relevant values.\n",
    "    \n",
    "    return output, attention_weights\n",
    "    # Purpose: Return attention output and weights.\n",
    "    # Theory: Output is used in Transformer layers; weights are useful for analysis.\n",
    "\n",
    "# Test implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Test custom attention against PyTorch's implementation.\n",
    "    # Theory: Verifies correctness and numerical accuracy.\n",
    "    \n",
    "    # Input tensors\n",
    "    batch_size, seq_len, dim = 1, 3, 3\n",
    "    # Purpose: Define dimensions for test tensors.\n",
    "    # Theory: Small dimensions for simplicity, matching problem constraints.\n",
    "    \n",
    "    q = torch.randn(batch_size, seq_len, dim)\n",
    "    k = torch.randn(batch_size, seq_len, dim)\n",
    "    v = torch.randn(batch_size, seq_len, dim)\n",
    "    # Purpose: Generate random query, key, and value tensors.\n",
    "    # Theory: Shape (1, 3, 3) simulates a small batch for testing.\n",
    "    \n",
    "    # Test without mask\n",
    "    output_custom, weights_custom = scaled_dot_product_attention(q, k, v)\n",
    "    # Purpose: Compute attention with custom implementation.\n",
    "    # Theory: Tests core functionality without masking.\n",
    "    \n",
    "    output_pytorch = F.scaled_dot_product_attention(q, k, v)\n",
    "    # Purpose: Compute attention with PyTorch's implementation.\n",
    "    # Theory: Serves as ground truth for comparison.\n",
    "    \n",
    "    print(\"Custom Output:\", output_custom)\n",
    "    print(\"PyTorch Output:\", output_pytorch)\n",
    "    # Purpose: Print outputs for visual inspection.\n",
    "    # Theory: Allows checking numerical closeness.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Outputs do not match!\"\n",
    "    # Purpose: Verify numerical equivalence.\n",
    "    # Theory: Ensures custom implementation matches PyTorch within tolerance.\n",
    "    \n",
    "    print(\"Test without mask passed!\")\n",
    "    # Purpose: Confirm successful test.\n",
    "    # Theory: Validates core attention mechanism.\n",
    "    \n",
    "    # Test with causal mask\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).unsqueeze(0)\n",
    "    # Purpose: Create causal mask to prevent attending to future tokens.\n",
    "    # Theory: Upper triangular matrix with 1s above diagonal, shape (1, 3, 3).\n",
    "    \n",
    "    output_custom, weights_custom = scaled_dot_product_attention(q, k, v, mask)\n",
    "    # Purpose: Compute attention with custom implementation and mask.\n",
    "    # Theory: Tests masking functionality for autoregressive models.\n",
    "    \n",
    "    output_pytorch = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n",
    "    # Purpose: Compute attention with PyTorch's implementation and mask.\n",
    "    # Theory: Ground truth for masked attention.\n",
    "    \n",
    "    print(\"Custom Output (with mask):\", output_custom)\n",
    "    print(\"PyTorch Output (with mask):\", output_pytorch)\n",
    "    # Purpose: Print masked outputs.\n",
    "    # Theory: Verifies masking correctness.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Masked outputs do not match!\"\n",
    "    # Purpose: Verify numerical equivalence for masked case.\n",
    "    # Theory: Ensures masking is applied correctly.\n",
    "    \n",
    "    print(\"Test with causal mask passed!\")\n",
    "    # Purpose: Confirm successful masked test.\n",
    "    # Theory: Validates attention for causal settings (e.g., GPT).\n",
    "    \n",
    "    # Test with padding mask\n",
    "    padding_mask = torch.tensor([[[0, 0, 1]]], dtype=torch.bool)\n",
    "    # Purpose: Create padding mask to mask specific positions.\n",
    "    # Theory: Masks the last token in the sequence, shape (1, 1, 3).\n",
    "    \n",
    "    output_custom, weights_custom = scaled_dot_product_attention(q, k, v, padding_mask)\n",
    "    # Purpose: Compute attention with padding mask.\n",
    "    # Theory: Tests handling of padding in sequences.\n",
    "    \n",
    "    output_pytorch = F.scaled_dot_product_attention(q, k, v, attn_mask=padding_mask)\n",
    "    # Purpose: Compute PyTorch attention with padding mask.\n",
    "    # Theory: Ground truth for padding case.\n",
    "    \n",
    "    print(\"Custom Output (with padding mask):\", output_custom)\n",
    "    print(\"PyTorch Output (with padding mask):\", output_pytorch)\n",
    "    # Purpose: Print padding-masked outputs.\n",
    "    # Theory: Verifies padding mask application.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Padding masked outputs do not match!\"\n",
    "    # Purpose: Verify numerical equivalence for padding mask.\n",
    "    # Theory: Ensures correct handling of padding.\n",
    "    \n",
    "    print(\"Test with padding mask passed!\")\n",
    "    # Purpose: Confirm successful padding test.\n",
    "    # Theory: Validates attention for padded sequences (e.g., BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc7ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
