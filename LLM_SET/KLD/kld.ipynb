{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39743854",
   "metadata": {},
   "source": [
    "# KL Divergence Loss Implementation\n",
    "\n",
    "## Problem Analysis: Implement KL Divergence Loss\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "You are tasked with implementing the **Kullback-Leibler (KL) Divergence Loss** as a custom loss function in PyTorch, a key component in training LLMs for tasks like knowledge distillation or aligning model outputs with a target distribution. KL Divergence measures the difference between two probability distributions, often used to encourage a model's predicted probabilities to match a target distribution.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "KL Divergence between two discrete probability distributions P (target) and Q (predicted) is defined as:\n",
    "\n",
    "```\n",
    "D_KL(P || Q) = ∑_i P(i) log(P(i)/Q(i))\n",
    "```\n",
    "\n",
    "For a batch of n samples and c classes, the loss is averaged:\n",
    "\n",
    "```\n",
    "L = (1/n) ∑_{j=1}^n ∑_{i=1}^c P_j(i) log(P_j(i)/Q_j(i))\n",
    "```\n",
    "\n",
    "where:\n",
    "- P_j(i): Target probability for sample j, class i\n",
    "- Q_j(i): Predicted probability for sample j, class i\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Implement a `KLDivergenceLoss` class inheriting from `torch.nn.Module`\n",
    "- Define the `forward` method to compute the KL Divergence Loss\n",
    "- Handle numerical stability (e.g., avoid division by zero or log of zero)\n",
    "- Integrate into a simple training pipeline with a synthetic dataset of probability distributions\n",
    "- Use PyTorch for tensor operations and autograd\n",
    "\n",
    "## Constraints\n",
    "\n",
    "- Use only PyTorch (no scikit-learn or other ML libraries)\n",
    "- Handle batch inputs (P, Q ∈ R^{n×c})\n",
    "- Ensure the loss is a scalar for optimization\n",
    "- Add a small constant (ε) to prevent numerical issues\n",
    "\n",
    "## Synthetic Dataset\n",
    "\n",
    "- Generate n=100 samples with c=10 classes\n",
    "- **Target (P)**: Softmax of random logits to simulate true probabilities\n",
    "- **Predicted (Q)**: Softmax of model outputs to simulate predicted probabilities\n",
    "- Test on a small batch to verify the loss\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "1. **Numerical Stability**: Add epsilon (ε = 1e-8) to prevent log(0) and division by 0\n",
    "2. **Probability Validation**: Ensure inputs are valid probability distributions\n",
    "3. **Batch Processing**: Handle multiple samples simultaneously\n",
    "4. **Gradient Flow**: Maintain differentiability for backpropagation\n",
    "\n",
    "### Expected Output Structure\n",
    "\n",
    "```python\n",
    "class KLDivergenceLoss(torch.nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        # Initialize with numerical stability constant\n",
    "        \n",
    "    def forward(self, target_probs, predicted_probs):\n",
    "        # Compute KL divergence loss\n",
    "        # Return scalar loss value\n",
    "```\n",
    "\n",
    "### Testing Requirements\n",
    "\n",
    "- Verify loss computation on synthetic data\n",
    "- Test gradient computation\n",
    "- Validate numerical stability\n",
    "- Compare with PyTorch's built-in KLDivLoss (if applicable)\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "# Create loss function\n",
    "kl_loss = KLDivergenceLoss()\n",
    "\n",
    "# Generate synthetic data\n",
    "target_probs = torch.softmax(torch.randn(100, 10), dim=1)\n",
    "predicted_probs = torch.softmax(torch.randn(100, 10), dim=1)\n",
    "\n",
    "# Compute loss\n",
    "loss = kl_loss(target_probs, predicted_probs)\n",
    "```\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Loss value should be non-negative\n",
    "- Loss should be 0 when P = Q\n",
    "- Gradient should flow properly for optimization\n",
    "- Should handle edge cases gracefully\n",
    "\n",
    "## Expected Deliverables\n",
    "\n",
    "1. Complete `KLDivergenceLoss` class implementation\n",
    "2. Synthetic dataset generation code\n",
    "3. Training pipeline integration\n",
    "4. Test cases and validation\n",
    "5. Documentation and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations and neural network functionality.\n",
    "# Theory: PyTorch provides tensors with GPU support and autograd for automatic differentiation, essential for custom loss functions and model training.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules, including nn.Module for defining custom loss and model classes.\n",
    "# Theory: nn.Module enables custom loss functions like KL Divergence to integrate with PyTorch’s autograd system.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Purpose: Import optimization algorithms like Adam for updating model parameters.\n",
    "# Theory: Adam adapts learning rates using momentum, suitable for optimizing models with custom losses.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix the random seed to ensure consistent random number generation.\n",
    "# Theory: Ensures reproducibility of synthetic data and model initialization, aligning with previous TorchLeet problems (e.g., DNN regression).\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples, n_classes = 100, 10\n",
    "# Purpose: Define the number of samples (100) and classes (10) for the synthetic dataset.\n",
    "# Theory: A dataset with multiple classes simulates probability distributions, suitable for testing KL Divergence.\n",
    "\n",
    "X = torch.rand(n_samples, n_classes)\n",
    "# Purpose: Generate random input logits for the model, shape [100, 10].\n",
    "# Theory: Random logits represent unnormalized scores, which will be converted to probabilities via softmax.\n",
    "\n",
    "y_true = torch.softmax(torch.rand(n_samples, n_classes), dim=1)\n",
    "# Purpose: Generate target probability distributions using softmax, shape [100, 10].\n",
    "# Theory: Softmax ensures valid probabilities (\\sum_i P(i) = 1). Random logits simulate true distributions from a teacher model.\n",
    "\n",
    "# Define the KL Divergence Loss\n",
    "class KLDivergenceLoss(nn.Module):\n",
    "    # Purpose: Define a custom KL Divergence Loss by subclassing nn.Module.\n",
    "    # Theory: nn.Module integrates the loss with PyTorch’s autograd, enabling gradient computation for optimization.\n",
    "    \n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        # Purpose: Initialize the loss with a small constant for numerical stability.\n",
    "        # Theory: epsilon prevents division by zero or log(0) in KL Divergence computation.\n",
    "        \n",
    "        super(KLDivergenceLoss, self).__init__()\n",
    "        # Purpose: Call the parent nn.Module constructor to set up the module.\n",
    "        # Theory: Ensures proper initialization for autograd integration.\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        # Purpose: Store epsilon as an instance variable.\n",
    "        # Theory: Used to stabilize log computations, a common practice in probabilistic losses.\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Purpose: Compute the KL Divergence Loss between predicted and true probability distributions.\n",
    "        # Theory: Computes D_KL(P || Q) = \\sum P(i) \\log(P(i)/Q(i)), averaged over samples. y_pred and y_true are [batch_size, n_classes].\n",
    "        \n",
    "        y_pred = torch.softmax(y_pred, dim=1)\n",
    "        # Purpose: Convert predicted logits to probabilities using softmax.\n",
    "        # Theory: Softmax ensures \\sum_i Q(i) = 1, making y_pred a valid probability distribution.\n",
    "        \n",
    "        y_pred = torch.clamp(y_pred, min=self.epsilon)\n",
    "        # Purpose: Clip predicted probabilities to avoid log(0) or division by zero.\n",
    "        # Theory: Clamping adds numerical stability, ensuring all values are positive.\n",
    "        \n",
    "        kl_div = y_true * (torch.log(y_true + self.epsilon) - torch.log(y_pred))\n",
    "        # Purpose: Compute the KL Divergence term P(i) \\log(P(i)/Q(i)) element-wise.\n",
    "        # Theory: Expands to P(i) (\\log P(i) - \\log Q(i)). Adding epsilon to y_true prevents log(0) in rare cases.\n",
    "        \n",
    "        kl_div = torch.sum(kl_div, dim=1)\n",
    "        # Purpose: Sum KL Divergence over classes for each sample.\n",
    "        # Theory: Aggregates the divergence across all classes, producing a per-sample loss [batch_size].\n",
    "        \n",
    "        return torch.mean(kl_div)\n",
    "        # Purpose: Average the loss over the batch to produce a scalar.\n",
    "        # Theory: Scalar loss is required for optimization, enabling gradient backpropagation.\n",
    "\n",
    "# Define a simple model to generate predicted probabilities\n",
    "class SimpleModel(nn.Module):\n",
    "    # Purpose: Define a simple neural network to output logits for KL Divergence.\n",
    "    # Theory: A linear layer maps inputs to logits, which are converted to probabilities via softmax.\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Purpose: Initialize the parent nn.Module class.\n",
    "        # Theory: Ensures proper parameter registration and autograd setup.\n",
    "        \n",
    "        self.linear = nn.Linear(n_classes, n_classes)\n",
    "        # Purpose: Create a linear layer mapping input logits to output logits.\n",
    "        # Theory: Applies z = Wx + b, where W is [n_classes, n_classes], b is [n_classes]. Outputs logits for softmax.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Purpose: Define the forward pass to compute logits.\n",
    "        # Theory: Builds the computational graph for autograd, producing logits for KL Divergence.\n",
    "        \n",
    "        return self.linear(x)\n",
    "        # Purpose: Apply the linear transformation.\n",
    "        # Theory: Outputs logits [batch_size, n_classes], which KLDivergenceLoss converts to probabilities.\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleModel()\n",
    "# Purpose: Create an instance of the model.\n",
    "# Theory: Initializes weights and biases randomly (Xavier initialization), tracked by autograd.\n",
    "\n",
    "criterion = KLDivergenceLoss()\n",
    "# Purpose: Initialize the KL Divergence Loss with default epsilon.\n",
    "# Theory: Prepares the custom loss for training, handling numerical stability.\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Purpose: Initialize Adam optimizer with learning rate 0.01.\n",
    "# Theory: Adam adapts learning rates using momentum (β1=0.9, β2=0.999), suitable for optimizing probabilistic models.\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "# Purpose: Set the number of training iterations to 1000 epochs.\n",
    "# Theory: Multiple epochs allow the model to learn to match the target distribution, consistent with previous TorchLeet problems.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Purpose: Iterate over the dataset for training.\n",
    "    # Theory: Each epoch processes the entire dataset, updating parameters to minimize KL Divergence.\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    # Purpose: Compute model logits by passing input X through the model.\n",
    "    # Theory: X [100, 10] produces logits [100, 10], which are converted to probabilities in the loss.\n",
    "    \n",
    "    loss = criterion(predictions, y_true)\n",
    "    # Purpose: Calculate the KL Divergence Loss between predicted and true distributions.\n",
    "    # Theory: Computes D_KL(P || Q), averaging over samples. Both tensors are [100, 10].\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    # Purpose: Reset gradients of all model parameters.\n",
    "    # Theory: Prevents gradient accumulation from previous iterations, ensuring correct updates.\n",
    "    \n",
    "    loss.backward()\n",
    "    # Purpose: Compute gradients of the loss with respect to model parameters.\n",
    "    # Theory: Autograd backpropagates through the loss (KL Divergence) → softmax → linear layer.\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Purpose: Update model parameters using gradients.\n",
    "    # Theory: Adam applies adaptive updates to minimize the loss.\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Purpose: Print training progress.\n",
    "        # Theory: Monitoring loss helps assess convergence and detect issues like numerical instability.\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        # Purpose: Display epoch and loss value.\n",
    "        # Theory: loss.item() extracts the scalar loss for readable output.\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.rand(2, n_classes)\n",
    "# Purpose: Generate a small test set with 2 samples.\n",
    "# Theory: Tests model generalization on new inputs, shape [2, 10].\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Purpose: Disable gradient tracking for inference.\n",
    "    # Theory: Saves memory and computation during evaluation.\n",
    "    \n",
    "    predictions = torch.softmax(model(X_test), dim=1)\n",
    "    # Purpose: Compute predicted probabilities for test inputs.\n",
    "    # Theory: Softmax converts logits to probabilities, shape [2, 10].\n",
    "    \n",
    "    print(f\"Test Predictions: {predictions.tolist()}\")\n",
    "    # Purpose: Print predicted probabilities.\n",
    "    # Theory: Shows how closely the model matches a target distribution (not directly comparable here due to random test data)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
