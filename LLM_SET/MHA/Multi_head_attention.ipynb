{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89565e1",
   "metadata": {},
   "source": [
    "# Implement Multi-Head Attention from Scratch\n",
    "\n",
    "## Description: Implement Multi-Head Attention (MHA), a core component of Transformer models, as described in “Attention is All You Need” (Vaswani et al., 2017). The function multi_head_attention(q, k, v, num_heads, d_model, mask=None) projects query ($   Q   $), key ($   K   $), and value ($   V   $) tensors into multiple attention heads, applies scaled dot-product attention per head, concatenates the outputs, and applies a final linear projection. The implementation must match PyTorch’s torch.nn.MultiheadAttention in output accuracy (within tolerance atol=1e-08, rtol=1e-05), handle batch dimensions, support masking, and use only PyTorch operations. The provided code fails the assertion due to mismatched weight initialization and masking logic. We’ll fix this by initializing weights to match PyTorch’s module and ensuring proper mask broadcasting.\n",
    "\n",
    "## Mathematical Definition:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "Query: $   Q \\in \\mathbb{R}^{N \\times L_q \\times d_{\\text{model}}}   $, where $   N   $ is batch size, $   L_q   $ is query sequence length, $   d_{\\text{model}}   $ is embedding dimension.\n",
    "Key: $   K \\in \\mathbb{R}^{N \\times L_k \\times d_{\\text{model}}}   $, where $   L_k   $ is key sequence length.\n",
    "Value: $   V \\in \\mathbb{R}^{N \\times L_k \\times d_{\\text{model}}}   $.\n",
    "Mask: Optional tensor $   M \\in \\mathbb{R}^{N \\times L_q \\times L_k}   $ (or broadcastable) with 0s (valid) or 1s (masked).\n",
    "$   \\text{num\\_heads}   $: Number of attention heads.\n",
    "$   d_{\\text{head}} = d_{\\text{model}} / \\text{num\\_heads}   $: Dimension per head.\n",
    "\n",
    "\n",
    "Linear Projections:\n",
    "\n",
    "Project inputs: $   Q_h = Q W_i^Q   $, $   K_h = K W_i^K   $, $   V_h = V W_i^V   $, where $   W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{head}}}   $ for each head $   i   $.\n",
    "Combine projections across heads: Shape becomes $   (N, \\text{num\\_heads}, L_q, d_{\\text{head}})   $ for $   Q_h   $.\n",
    "\n",
    "\n",
    "Scaled Dot-Product Attention (per head):\n",
    "\n",
    "Scores: $   S = Q_h K_h^T / \\sqrt{d_{\\text{head}}}   $, where $   S \\in \\mathbb{R}^{N \\times \\text{num\\_heads} \\times L_q \\times L_k}   $.\n",
    "Mask: Set $   S_{i,j} = -\\infty   $ where $   M_{i,j} = 1   $.\n",
    "Weights: $   A = \\text{softmax}(S, \\text{dim}=-1)   $.\n",
    "Output: $   O_h = A V_h   $, where $   O_h \\in \\mathbb{R}^{N \\times \\text{num\\_heads} \\times L_q \\times d_{\\text{head}}}   $.\n",
    "\n",
    "\n",
    "Concatenation and Final Projection:\n",
    "\n",
    "Concatenate heads: $   O = \\text{Concat}(O_1, \\ldots, O_{\\text{num\\_heads}})   $, shape $   (N, L_q, d_{\\text{model}})   $.\n",
    "Final projection: $   O_{\\text{final}} = O W^O   $, where $   W^O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}   $.\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Return $   O_{\\text{final}} \\in \\mathbb{R}^{N \\times L_q \\times d_{\\text{model}}}   $.\n",
    "\n",
    "\n",
    "Validation:\n",
    "\n",
    "Compare with torch.nn.MultiheadAttention using torch.allclose.\n",
    "\n",
    "\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Implement multi_head_attention(q, k, v, num_heads, d_model, mask=None):\n",
    "\n",
    "Inputs: $   Q, K, V   $ of shape $  (N, L_q, d_{\\text{model}})  $, $  (N, L_k, d_{\\text{model}})  $, $  (N, L_k, d_{\\text{model}})  $, number of heads, model dimension, and optional mask.\n",
    "Output: Attention output of shape $  (N, L_q, d_{\\text{model}})  $.\n",
    "Support batch processing and masking (causal or padding).\n",
    "\n",
    "\n",
    "Use PyTorch operations (torch.matmul, F.softmax, nn.Linear).\n",
    "Test with synthetic tensors ($   N=3, L_q=L_k=4, d_{\\text{model}}=8, \\text{num\\_heads}=2   $).\n",
    "Match PyTorch’s torch.nn.MultiheadAttention output.\n",
    "Provide detailed Purpose and Theory comments.\n",
    "Fix the provided code’s assertion failure by aligning weight initialization and mask handling.\n",
    "\n",
    "Constraints:\n",
    "\n",
    "Use only PyTorch operations (no transformers or external libraries).\n",
    "Ensure $   d_{\\text{model}}   $ is divisible by num_heads.\n",
    "Support batch-first format ($   (N, L_q, d_{\\text{model}})   $).\n",
    "Handle optional masking for causal or padding scenarios.\n",
    "Match PyTorch’s output within atol=1e-08, rtol=1e-05.\n",
    "\n",
    "Synthetic Dataset:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "$   Q, K, V   $: Random tensors of shape $  (3, 4, 8)  $, generated with torch.rand and seed 42.\n",
    "$   \\text{num\\_heads} = 2   $, $   d_{\\text{model}} = 8   $, so $   d_{\\text{head}} = 4   $.\n",
    "Mask: Test with no mask, causal mask, and padding mask.\n",
    "\n",
    "\n",
    "Test Cases:\n",
    "\n",
    "Without mask.\n",
    "Causal mask (upper triangular) to prevent attending to future tokens.\n",
    "Padding mask to ignore specific positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff497a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Output: tensor([[[ 7.1700e-02,  5.3411e-02,  3.3034e-01,  1.5892e-01,  6.5924e-02,\n",
      "           2.3432e-01,  1.2263e-01,  2.1005e-01],\n",
      "         [ 7.5689e-02,  5.2544e-02,  3.3219e-01,  1.5638e-01,  6.8600e-02,\n",
      "           2.3832e-01,  1.2223e-01,  2.1144e-01],\n",
      "         [ 7.2534e-02,  5.3125e-02,  3.3081e-01,  1.5864e-01,  6.6714e-02,\n",
      "           2.3542e-01,  1.2263e-01,  2.1010e-01],\n",
      "         [ 6.9490e-02,  5.1792e-02,  3.3143e-01,  1.6153e-01,  6.6810e-02,\n",
      "           2.3345e-01,  1.2220e-01,  2.0941e-01]],\n",
      "\n",
      "        [[-4.5193e-02,  2.6360e-02,  2.6671e-01,  2.7284e-01,  9.3438e-02,\n",
      "           1.5845e-01,  1.2320e-01,  1.5793e-01],\n",
      "         [-4.3427e-02,  2.6275e-02,  2.6286e-01,  2.7077e-01,  9.2779e-02,\n",
      "           1.5760e-01,  1.2343e-01,  1.5535e-01],\n",
      "         [-4.5840e-02,  2.6753e-02,  2.6641e-01,  2.7329e-01,  9.4146e-02,\n",
      "           1.5864e-01,  1.2388e-01,  1.5618e-01],\n",
      "         [-4.1810e-02,  2.5378e-02,  2.6161e-01,  2.6910e-01,  9.1674e-02,\n",
      "           1.5723e-01,  1.2250e-01,  1.5550e-01]],\n",
      "\n",
      "        [[ 1.2124e-02,  9.1738e-03,  2.8307e-01,  2.1436e-01,  3.8322e-02,\n",
      "           1.6121e-01,  1.4600e-01,  4.0203e-02],\n",
      "         [ 1.2860e-02,  1.0159e-02,  2.8353e-01,  2.1204e-01,  3.6763e-02,\n",
      "           1.5970e-01,  1.4690e-01,  3.9122e-02],\n",
      "         [ 2.3368e-04,  1.0734e-02,  2.7735e-01,  2.2325e-01,  3.2398e-02,\n",
      "           1.5060e-01,  1.4536e-01,  3.8573e-02],\n",
      "         [-1.0008e-03,  1.0541e-02,  2.7713e-01,  2.2460e-01,  3.1621e-02,\n",
      "           1.4955e-01,  1.4590e-01,  3.6826e-02]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "PyTorch Output: tensor([[[ 7.1700e-02,  5.3411e-02,  3.3034e-01,  1.5892e-01,  6.5924e-02,\n",
      "           2.3432e-01,  1.2263e-01,  2.1005e-01],\n",
      "         [ 7.5689e-02,  5.2544e-02,  3.3219e-01,  1.5638e-01,  6.8600e-02,\n",
      "           2.3832e-01,  1.2223e-01,  2.1144e-01],\n",
      "         [ 7.2534e-02,  5.3125e-02,  3.3081e-01,  1.5864e-01,  6.6714e-02,\n",
      "           2.3542e-01,  1.2263e-01,  2.1010e-01],\n",
      "         [ 6.9490e-02,  5.1792e-02,  3.3143e-01,  1.6153e-01,  6.6810e-02,\n",
      "           2.3345e-01,  1.2220e-01,  2.0941e-01]],\n",
      "\n",
      "        [[-4.5193e-02,  2.6360e-02,  2.6671e-01,  2.7284e-01,  9.3438e-02,\n",
      "           1.5845e-01,  1.2320e-01,  1.5793e-01],\n",
      "         [-4.3427e-02,  2.6275e-02,  2.6286e-01,  2.7077e-01,  9.2779e-02,\n",
      "           1.5760e-01,  1.2343e-01,  1.5535e-01],\n",
      "         [-4.5840e-02,  2.6753e-02,  2.6641e-01,  2.7329e-01,  9.4146e-02,\n",
      "           1.5864e-01,  1.2388e-01,  1.5618e-01],\n",
      "         [-4.1810e-02,  2.5378e-02,  2.6161e-01,  2.6910e-01,  9.1674e-02,\n",
      "           1.5723e-01,  1.2250e-01,  1.5550e-01]],\n",
      "\n",
      "        [[ 1.2124e-02,  9.1738e-03,  2.8307e-01,  2.1436e-01,  3.8322e-02,\n",
      "           1.6121e-01,  1.4600e-01,  4.0203e-02],\n",
      "         [ 1.2860e-02,  1.0159e-02,  2.8353e-01,  2.1204e-01,  3.6763e-02,\n",
      "           1.5970e-01,  1.4690e-01,  3.9122e-02],\n",
      "         [ 2.3368e-04,  1.0734e-02,  2.7735e-01,  2.2325e-01,  3.2398e-02,\n",
      "           1.5060e-01,  1.4536e-01,  3.8573e-02],\n",
      "         [-1.0008e-03,  1.0541e-02,  2.7713e-01,  2.2460e-01,  3.1621e-02,\n",
      "           1.4955e-01,  1.4590e-01,  3.6826e-02]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Test without mask passed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 3D attn_mask is torch.Size([3, 4, 4]), but should be (6, 4, 4).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 249\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Purpose: Create causal mask for autoregressive attention.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Theory: Shape (batch_size, seq_len, seq_len), masks future tokens.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m output_custom \u001b[38;5;241m=\u001b[39m multi_head_attention(q, k, v, num_heads, d_model, mask)\n\u001b[1;32m--> 249\u001b[0m output_pytorch, _ \u001b[38;5;241m=\u001b[39m multihead_attn(q, k, v, attn_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Purpose: Compute masked outputs.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Theory: Tests causal mask handling.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Output (with causal mask):\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_custom)\n",
      "File \u001b[1;32mc:\\Users\\nisar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nisar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nisar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1276\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1280\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1281\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1282\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1283\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1284\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1285\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\nisar\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5443\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5441\u001b[0m     correct_3d_size \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m num_heads, tgt_len, src_len)\n\u001b[0;32m   5442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_3d_size:\n\u001b[1;32m-> 5443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 3D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_3d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The shape of the 3D attn_mask is torch.Size([3, 4, 4]), but should be (6, 4, 4)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations.\n",
    "# Theory: Provides tensor computations with autograd support for attention.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules for linear projections.\n",
    "# Theory: nn.Linear is used for Q, K, V projections and output transformation.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# Purpose: Import functional module for softmax.\n",
    "# Theory: F.softmax computes attention weights over the key dimension.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix random seed for consistent input tensors and weight initialization.\n",
    "# Theory: Ensures reproducible results, aligning with previous problems (e.g., Scaled Dot-Product Attention).\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention (from previous problem).\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor of shape (..., seq_len_q, d_k)\n",
    "        k: Key tensor of shape (..., seq_len_k, d_k)\n",
    "        v: Value tensor of shape (..., seq_len_k, d_v)\n",
    "        mask: Optional mask tensor of shape (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output tensor of shape (..., seq_len_q, d_v)\n",
    "        attention_weights: Attention weights tensor of shape (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # Purpose: Define scaled dot-product attention for use in multi-head attention.\n",
    "    # Theory: Computes attention scores, applies mask, and weights values.\n",
    "    \n",
    "    d_k = q.size(-1)\n",
    "    # Purpose: Get key dimension for scaling.\n",
    "    # Theory: Used to scale scores by sqrt(d_k) for numerical stability.\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "    # Purpose: Compute dot product: Q * K^T.\n",
    "    # Theory: Produces raw scores, shape (..., seq_len_q, seq_len_k).\n",
    "    \n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    # Purpose: Scale scores by sqrt(d_k).\n",
    "    # Theory: Prevents large dot products, stabilizing gradients.\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Purpose: Apply optional mask.\n",
    "        # Theory: Masks future tokens or padding, setting scores to -inf.\n",
    "        \n",
    "        scores = scores.masked_fill(mask == 1, -1e9)\n",
    "        # Purpose: Set masked positions to large negative value.\n",
    "        # Theory: Ensures near-zero weights after softmax.\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    # Purpose: Convert scores to probabilities.\n",
    "    # Theory: Softmax over key dimension ensures weights sum to 1.\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    # Purpose: Compute weighted sum: A * V.\n",
    "    # Theory: Produces output, shape (..., seq_len_q, d_v).\n",
    "    \n",
    "    return output, attention_weights\n",
    "    # Purpose: Return attention output and weights.\n",
    "    # Theory: Output for further processing, weights for analysis.\n",
    "\n",
    "def multi_head_attention(q, k, v, num_heads, d_model, mask=None):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        k: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        v: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        num_heads: Number of attention heads\n",
    "        d_model: Total embedding dimension\n",
    "        mask: Optional mask tensor for attention\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Multi-head attention output of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # Purpose: Define multi-head attention function.\n",
    "    # Theory: Projects inputs, applies attention per head, concatenates, and projects output.\n",
    "    \n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    # Purpose: Ensure valid head dimension.\n",
    "    # Theory: d_head = d_model / num_heads must be an integer.\n",
    "    \n",
    "    batch_size, seq_len, _ = q.shape\n",
    "    # Purpose: Get input dimensions.\n",
    "    # Theory: batch_size and seq_len are used for reshaping tensors.\n",
    "    \n",
    "    d_head = d_model // num_heads\n",
    "    # Purpose: Compute dimension per head.\n",
    "    # Theory: Each head processes a subspace of d_model.\n",
    "    \n",
    "    # Initialize linear layers\n",
    "    q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    out_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    # Purpose: Define linear projections for Q, K, V, and output.\n",
    "    # Theory: Projects inputs to head-specific subspaces and combines outputs.\n",
    "    \n",
    "    # Project inputs\n",
    "    Q = q_linear(q)\n",
    "    K = k_linear(k)\n",
    "    V = v_linear(v)\n",
    "    # Purpose: Apply linear projections to Q, K, V.\n",
    "    # Theory: Shape (batch_size, seq_len, d_model), preparing for head splitting.\n",
    "    \n",
    "    # Reshape for multi-head: (batch_size, seq_len, num_heads, d_head) -> (batch_size, num_heads, seq_len, d_head)\n",
    "    Q = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    K = K.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    V = V.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    # Purpose: Split projections into heads and reorder dimensions.\n",
    "    # Theory: Shape (batch_size, num_heads, seq_len, d_head) for parallel attention.\n",
    "    \n",
    "    # Adjust mask for multi-head\n",
    "    if mask is not None:\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len)\n",
    "        # Purpose: Ensure mask is compatible with multi-head shape.\n",
    "        # Theory: Broadcasts mask to (batch_size, num_heads, seq_len, seq_len).\n",
    "    \n",
    "    # Apply scaled dot-product attention\n",
    "    output, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "    # Purpose: Compute attention for each head.\n",
    "    # Theory: Output shape (batch_size, num_heads, seq_len, d_head).\n",
    "    \n",
    "    # Reshape output: (batch_size, num_heads, seq_len, d_head) -> (batch_size, seq_len, d_model)\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "    # Purpose: Concatenate heads and flatten to original shape.\n",
    "    # Theory: Combines head outputs, shape (batch_size, seq_len, d_model).\n",
    "    \n",
    "    # Final projection\n",
    "    output = out_linear(output)\n",
    "    # Purpose: Apply final linear transformation.\n",
    "    # Theory: Restores output to (batch_size, seq_len, d_model).\n",
    "    \n",
    "    return output\n",
    "    # Purpose: Return final attention output.\n",
    "    # Theory: Matches input shape for use in Transformer layers.\n",
    "\n",
    "# Test implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Test multi-head attention against PyTorch's implementation.\n",
    "    # Theory: Verifies numerical accuracy and mask handling.\n",
    "    \n",
    "    # Input tensors\n",
    "    batch_size, seq_len, d_model, num_heads = 3, 4, 8, 2\n",
    "    # Purpose: Define dimensions for test tensors.\n",
    "    # Theory: Matches problem constraints for synthetic data.\n",
    "    \n",
    "    q = torch.rand(batch_size, seq_len, d_model)\n",
    "    k = torch.rand(batch_size, seq_len, d_model)\n",
    "    v = torch.rand(batch_size, seq_len, d_model)\n",
    "    # Purpose: Generate random input tensors.\n",
    "    # Theory: Shape (3, 4, 8) for batch processing.\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Purpose: Set device for computation.\n",
    "    # Theory: Ensures compatibility with GPU if available.\n",
    "    \n",
    "    q, k, v = q.to(device), k.to(device), v.to(device)\n",
    "    # Purpose: Move tensors to device.\n",
    "    # Theory: Ensures computations run on the same device.\n",
    "    \n",
    "    # Initialize PyTorch's MultiheadAttention\n",
    "    multihead_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True).to(device)\n",
    "    # Purpose: Create reference model for comparison.\n",
    "    # Theory: PyTorch’s implementation is the ground truth.\n",
    "    \n",
    "    # Copy weights to custom linear layers\n",
    "    custom_q_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    custom_k_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    custom_v_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    custom_out_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    # Purpose: Initialize custom linear layers.\n",
    "    # Theory: Must match PyTorch’s weights to pass assertion.\n",
    "    \n",
    "    # Access PyTorch's internal weights\n",
    "    with torch.no_grad():\n",
    "        # PyTorch combines Q, K, V projections into one matrix\n",
    "        in_proj_weight = multihead_attn.in_proj_weight\n",
    "        # Split into Q, K, V weights\n",
    "        q_weight = in_proj_weight[:d_model, :]\n",
    "        k_weight = in_proj_weight[d_model:2*d_model, :]\n",
    "        v_weight = in_proj_weight[2*d_model:, :]\n",
    "        # Assign to custom layers\n",
    "        custom_q_linear.weight.copy_(q_weight)\n",
    "        custom_k_linear.weight.copy_(k_weight)\n",
    "        custom_v_linear.weight.copy_(v_weight)\n",
    "        custom_out_linear.weight.copy_(multihead_attn.out_proj.weight)\n",
    "    # Purpose: Copy weights from PyTorch’s module to custom layers.\n",
    "    # Theory: Ensures identical projections to match output.\n",
    "    \n",
    "    # Redefine multi_head_attention with custom weights\n",
    "    def multi_head_attention(q, k, v, num_heads, d_model, mask=None):\n",
    "        # Purpose: Redefine function with custom weights for testing.\n",
    "        # Theory: Same logic as above, using initialized weights.\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        batch_size, seq_len, _ = q.shape\n",
    "        d_head = d_model // num_heads\n",
    "        \n",
    "        Q = custom_q_linear(q)\n",
    "        K = custom_k_linear(k)\n",
    "        V = custom_v_linear(v)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "        \n",
    "        output, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return custom_out_linear(output)\n",
    "    \n",
    "    # Test without mask\n",
    "    output_custom = multi_head_attention(q, k, v, num_heads, d_model)\n",
    "    output_pytorch, _ = multihead_attn(q, k, v)\n",
    "    # Purpose: Compute outputs for comparison.\n",
    "    # Theory: Tests core multi-head attention functionality.\n",
    "    \n",
    "    print(\"Custom Output:\", output_custom)\n",
    "    print(\"PyTorch Output:\", output_pytorch)\n",
    "    # Purpose: Print outputs for inspection.\n",
    "    # Theory: Allows visual confirmation of similarity.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Outputs do not match!\"\n",
    "    # Purpose: Verify numerical equivalence.\n",
    "    # Theory: Ensures custom implementation matches PyTorch.\n",
    "    \n",
    "    print(\"Test without mask passed!\")\n",
    "    # Purpose: Confirm successful test.\n",
    "    # Theory: Validates core functionality.\n",
    "    \n",
    "    # Test with causal mask\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).unsqueeze(0).expand(batch_size, -1, -1).to(device)\n",
    "    # Purpose: Create causal mask for autoregressive attention.\n",
    "    # Theory: Shape (batch_size, seq_len, seq_len), masks future tokens.\n",
    "    \n",
    "    output_custom = multi_head_attention(q, k, v, num_heads, d_model, mask)\n",
    "    output_pytorch, _ = multihead_attn(q, k, v, attn_mask=mask)\n",
    "    # Purpose: Compute masked outputs.\n",
    "    # Theory: Tests causal mask handling.\n",
    "    \n",
    "    print(\"Custom Output (with causal mask):\", output_custom)\n",
    "    print(\"PyTorch Output (with causal mask):\", output_pytorch)\n",
    "    # Purpose: Print masked outputs.\n",
    "    # Theory: Verifies mask application.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Causal masked outputs do not match!\"\n",
    "    # Purpose: Verify masked output equivalence.\n",
    "    # Theory: Ensures correct causal attention.\n",
    "    \n",
    "    print(\"Test with causal mask passed!\")\n",
    "    # Purpose: Confirm successful masked test.\n",
    "    # Theory: Validates autoregressive attention (e.g., GPT).\n",
    "    \n",
    "    # Test with padding mask\n",
    "    padding_mask = torch.tensor([[[0, 0, 0, 1],\n",
    "                                 [0, 0, 0, 1],\n",
    "                                 [0, 0, 0, 1],\n",
    "                                 [0, 0, 0, 1]],\n",
    "                                [[0, 0, 0, 0],\n",
    "                                 [0, 0, 0, 0],\n",
    "                                 [0, 0, 0, 0],\n",
    "                                 [0, 0, 0, 0]],\n",
    "                                [[0, 0, 1, 1],\n",
    "                                 [0, 0, 1, 1],\n",
    "                                 [0, 0, 1, 1],\n",
    "                                 [0, 0, 1, 1]]], dtype=torch.bool).to(device)\n",
    "    # Purpose: Create padding mask for specific positions.\n",
    "    # Theory: Shape (batch_size, seq_len, seq_len), masks padded tokens.\n",
    "    \n",
    "    output_custom = multi_head_attention(q, k, v, num_heads, d_model, padding_mask)\n",
    "    output_pytorch, _ = multihead_attn(q, k, v, attn_mask=padding_mask)\n",
    "    # Purpose: Compute padding-masked outputs.\n",
    "    # Theory: Tests padding mask handling.\n",
    "    \n",
    "    print(\"Custom Output (with padding mask):\", output_custom)\n",
    "    print(\"PyTorch Output (with padding mask):\", output_pytorch)\n",
    "    # Purpose: Print padding-masked outputs.\n",
    "    # Theory: Verifies padding mask application.\n",
    "    \n",
    "    assert torch.allclose(output_custom, output_pytorch, atol=1e-08, rtol=1e-05), \"Padding masked outputs do not match!\"\n",
    "    # Purpose: Verify padding-masked output equivalence.\n",
    "    # Theory: Ensures correct handling of padded sequences (e.g., BERT).\n",
    "    \n",
    "    print(\"Test with padding mask passed!\")\n",
    "    # Purpose: Confirm successful padding test.\n",
    "    # Theory: Validates attention for padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d50d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
