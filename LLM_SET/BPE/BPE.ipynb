{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e7b092",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "Title: Implement Byte Pair Encoding (BPE) from Scratch\n",
    "\n",
    "Description: You are tasked with implementing Byte Pair Encoding (BPE), a subword tokenization algorithm used in Large Language Models (LLMs) like GPT and BERT to create a vocabulary of subword units. BPE iteratively merges the most frequent pair of adjacent characters or subwords in a text corpus to build a vocabulary, balancing between character-level and word-level representations. Your implementation should process a small text corpus, generate a vocabulary, and encode/decode text using the learned merges. The solution should be in pure Python (no PyTorch or NumPy required, as BPE is a text-processing algorithm), and include detailed comments explaining each step.\n",
    "\n",
    "Mathematical Definition:\n",
    "\n",
    "Vocabulary Creation:\n",
    "Start with a character-level vocabulary (e.g., individual characters in the corpus).\n",
    "Compute the frequency of all adjacent pairs of tokens (characters or subwords).\n",
    "Merge the most frequent pair into a new token, updating the corpus.\n",
    "Repeat for a specified number of merges (e.g., 100).\n",
    "Encoding:\n",
    "For a given word, split into characters.\n",
    "Apply learned merge rules in order to form subword tokens.\n",
    "Output a sequence of token IDs from the vocabulary.\n",
    "Decoding:\n",
    "Convert token IDs back to subword tokens.\n",
    "Concatenate subwords to reconstruct the original text.\n",
    "Requirements:\n",
    "\n",
    "Implement a BPE class with methods for:\n",
    "train: Learn merge rules from a text corpus.\n",
    "encode: Convert text to a sequence of token IDs.\n",
    "decode: Convert token IDs back to text.\n",
    "Use a small synthetic corpus (e.g., a few sentences) to train the BPE model.\n",
    "Generate a vocabulary with 100 merges.\n",
    "Handle edge cases (e.g., unknown characters, empty inputs).\n",
    "Provide detailed Purpose and Theory comments for each line of code.\n",
    "Test encoding and decoding on sample texts to verify correctness.\n",
    "Constraints:\n",
    "\n",
    "Use only Python standard libraries (e.g., collections, re).\n",
    "No external libraries like tokenizers or sentencepiece.\n",
    "Process text at the character level initially.\n",
    "Ensure encoding/decoding is reversible (lossless).\n",
    "Vocabulary size = initial characters + 100 merges.\n",
    "Synthetic Dataset:\n",
    "\n",
    "Corpus: A small text corpus (e.g., [\"hello world\", \"hello there\", \"world peace\", \"hi there\"]).\n",
    "Test Inputs: Words like \"hello\", \"world\", \"hi\", and an unseen word \"hellothere\".\n",
    "Vocabulary: Initial character set (e.g., {h, e, l, o, , w, r, d, t, p, a, c, i}) plus 100 merged tokens.\n",
    "Expected Output:\n",
    "\n",
    "Vocabulary: A dictionary mapping tokens (characters or subwords) to IDs, with ~113 tokens (13 initial + 100 merges).\n",
    "Merge Rules: A list of tuples (e.g., [(‘h’, ‘e’), (‘he’, ‘l’)]) defining merge operations.\n",
    "Encoded Output: Token IDs for test words (e.g., encode(\"hello\") → [7, 8, 9, 9, 10]).\n",
    "Decoded Output: Reconstructed text (e.g., decode([7, 8, 9, 9, 10]) → \"hello\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0389ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13\n",
      "Sample Merge Rules: [('h', 'e'), ('h', 'e'), ('h', 'e'), ('h', 'e')]\n",
      "Encoded 'hello': [13, 6, 6, 7]\n",
      "Decoded [13, 6, 6, 7]: hello\n",
      "Encoded 'hellothere': [13, 6, 6, 7, 10, 13, 9, 3]\n",
      "Decoded [13, 6, 6, 7, 10, 13, 9, 3]: hellothere\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "# Purpose: Import Counter for frequency counting and defaultdict for pair tracking.\n",
    "# Theory: Counter efficiently counts token pairs; defaultdict simplifies pair frequency aggregation.\n",
    "\n",
    "import re\n",
    "# Purpose: Import re for splitting text into words.\n",
    "# Theory: Regular expressions split text on whitespace, preserving words for BPE processing.\n",
    "\n",
    "class BPE:\n",
    "    # Purpose: Define BPE class for training, encoding, and decoding text.\n",
    "    # Theory: Encapsulates vocabulary, merge rules, and tokenization logic for LLMs.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Purpose: Initialize BPE with empty vocabulary and merge rules.\n",
    "        # Theory: Sets up data structures for training and tokenization.\n",
    "        \n",
    "        self.vocab = {}\n",
    "        # Purpose: Store token-to-ID mapping.\n",
    "        # Theory: Maps characters and merged subwords to unique integer IDs.\n",
    "        \n",
    "        self.merge_rules = []\n",
    "        # Purpose: Store list of merge rules as (token1, token2) tuples.\n",
    "        # Theory: Records the order of merges for consistent encoding.\n",
    "        \n",
    "        self.reverse_vocab = {}\n",
    "        # Purpose: Store ID-to-token mapping for decoding.\n",
    "        # Theory: Enables reverse lookup to reconstruct text from token IDs.\n",
    "    \n",
    "    def get_stats(self, word_counts):\n",
    "        # Purpose: Compute frequency of adjacent token pairs in the corpus.\n",
    "        # Theory: Identifies the most frequent pair for merging, a core step in BPE training.\n",
    "        \n",
    "        pairs = defaultdict(int)\n",
    "        # Purpose: Initialize dictionary to count pair frequencies.\n",
    "        # Theory: defaultdict(int) assigns 0 to new pairs, simplifying counting.\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            # Purpose: Iterate over words and their frequencies.\n",
    "            # Theory: Processes each word in the corpus, weighted by its frequency.\n",
    "            \n",
    "            tokens = list(word)\n",
    "            # Purpose: Split word into list of tokens (initially characters).\n",
    "            # Theory: Represents word as a sequence of tokens for pair analysis.\n",
    "            \n",
    "            for i in range(len(tokens) - 1):\n",
    "                # Purpose: Iterate over adjacent token pairs.\n",
    "                # Theory: Counts (t_i, t_{i+1}) pairs to find frequent combinations.\n",
    "                \n",
    "                pairs[(tokens[i], tokens[i + 1])] += count\n",
    "                # Purpose: Increment frequency of the pair (t_i, t_{i+1}).\n",
    "                # Theory: Aggregates pair counts across all occurrences in the corpus.\n",
    "        \n",
    "        return pairs\n",
    "        # Purpose: Return dictionary of pair frequencies.\n",
    "        # Theory: Provides data for selecting the most frequent pair to merge.\n",
    "    \n",
    "    def merge_pair(self, pair, word_counts):\n",
    "        # Purpose: Merge a given pair in all words, updating word_counts.\n",
    "        # Theory: Replaces adjacent occurrences of pair with a new token, advancing BPE training.\n",
    "        \n",
    "        new_token = pair[0] + pair[1]\n",
    "        # Purpose: Create new token by concatenating the pair.\n",
    "        # Theory: Combines two tokens (e.g., 'h', 'e' → 'he') to form a subword unit.\n",
    "        \n",
    "        new_word_counts = defaultdict(int)\n",
    "        # Purpose: Initialize new dictionary for updated word representations.\n",
    "        # Theory: Stores words after merging the pair, preserving frequencies.\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            # Purpose: Iterate over current word representations.\n",
    "            # Theory: Processes each word to apply the merge rule.\n",
    "            \n",
    "            tokens = list(word)\n",
    "            # Purpose: Convert word to list of tokens.\n",
    "            # Theory: Allows manipulation of token sequences for merging.\n",
    "            \n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            # Purpose: Initialize list for new token sequence after merging.\n",
    "            # Theory: Builds the updated word representation.\n",
    "            \n",
    "            while i < len(tokens):\n",
    "                # Purpose: Iterate through tokens to find and merge the pair.\n",
    "                # Theory: Scans for adjacent tokens matching the pair to merge.\n",
    "                \n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                    # Purpose: Check if current position matches the pair to merge.\n",
    "                    # Theory: Identifies occurrences of the pair (e.g., 'h', 'e').\n",
    "                    \n",
    "                    new_tokens.append(new_token)\n",
    "                    # Purpose: Add merged token to the new sequence.\n",
    "                    # Theory: Replaces pair with new token (e.g., 'he').\n",
    "                    \n",
    "                    i += 2\n",
    "                    # Purpose: Skip the merged pair.\n",
    "                    # Theory: Advances past both tokens to avoid re-processing.\n",
    "                else:\n",
    "                    # Purpose: Keep non-matching token as is.\n",
    "                    # Theory: Preserves tokens not involved in the merge.\n",
    "                    \n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_word = ''.join(new_tokens)\n",
    "            # Purpose: Join tokens into a new word representation.\n",
    "            # Theory: Converts token list back to a string for further processing.\n",
    "            \n",
    "            new_word_counts[new_word] += count\n",
    "            # Purpose: Update frequency of the new word representation.\n",
    "            # Theory: Maintains corpus frequency after merging.\n",
    "        \n",
    "        return new_word_counts\n",
    "        # Purpose: Return updated word counts after merging.\n",
    "        # Theory: Provides new corpus representation for the next merge iteration.\n",
    "    \n",
    "    def train(self, corpus, num_merges=100):\n",
    "        # Purpose: Train BPE model by learning merge rules from the corpus.\n",
    "        # Theory: Iteratively merges the most frequent token pairs to build vocabulary.\n",
    "        \n",
    "        # Split corpus into words and count frequencies\n",
    "        word_counts = Counter(' '.join(corpus).split())\n",
    "        # Purpose: Count frequency of each word in the corpus.\n",
    "        # Theory: Treats corpus as a single string, splits on whitespace to get words.\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        vocab = set()\n",
    "        for word in word_counts:\n",
    "            # Purpose: Collect unique characters from all words.\n",
    "            # Theory: Initial vocabulary includes all characters in the corpus.\n",
    "            \n",
    "            for char in word:\n",
    "                vocab.add(char)\n",
    "        \n",
    "        self.vocab = {char: i for i, char in enumerate(sorted(vocab))}\n",
    "        # Purpose: Assign IDs to initial characters.\n",
    "        # Theory: Creates a mapping from characters to unique IDs, starting the vocabulary.\n",
    "        \n",
    "        # Split words into character sequences\n",
    "        word_counts = {''.join(list(word)): count for word, count in word_counts.items()}\n",
    "        # Purpose: Represent words as character sequences for merging.\n",
    "        # Theory: Prepares words for token pair analysis, preserving frequencies.\n",
    "        \n",
    "        # Perform merges\n",
    "        for merge_idx in range(num_merges):\n",
    "            # Purpose: Iterate for the specified number of merges.\n",
    "            # Theory: Each iteration merges the most frequent pair, growing the vocabulary.\n",
    "            \n",
    "            pairs = self.get_stats(word_counts)\n",
    "            # Purpose: Compute pair frequencies in the current corpus.\n",
    "            # Theory: Identifies candidates for merging based on frequency.\n",
    "            \n",
    "            if not pairs:\n",
    "                # Purpose: Break if no pairs are available.\n",
    "                # Theory: Handles case where no further merges are possible (e.g., short words).\n",
    "                \n",
    "                break\n",
    "            \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            # Purpose: Select the most frequent pair to merge.\n",
    "            # Theory: Chooses the pair with the highest frequency for vocabulary expansion.\n",
    "            \n",
    "            word_counts = self.merge_pair(best_pair, word_counts)\n",
    "            # Purpose: Update corpus by merging the selected pair.\n",
    "            # Theory: Replaces all occurrences of the pair with a new token.\n",
    "            \n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            # Purpose: Create new token from the merged pair.\n",
    "            # Theory: Adds the merged token to the vocabulary.\n",
    "            \n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            # Purpose: Assign a new ID to the merged token.\n",
    "            # Theory: Expands vocabulary with the new subword unit.\n",
    "            \n",
    "            self.merge_rules.append(best_pair)\n",
    "            # Purpose: Record the merge rule.\n",
    "            # Theory: Stores the pair for use in encoding new text.\n",
    "        \n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        # Purpose: Create reverse mapping from IDs to tokens.\n",
    "        # Theory: Enables decoding by mapping token IDs back to strings.\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Purpose: Encode text into a sequence of token IDs.\n",
    "        # Theory: Applies learned merge rules to tokenize text into subword units.\n",
    "        \n",
    "        if not text:\n",
    "            # Purpose: Handle empty input.\n",
    "            # Theory: Returns empty list for empty text to avoid errors.\n",
    "            \n",
    "            return []\n",
    "        \n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        # Purpose: Split input text into words.\n",
    "        # Theory: Processes each word independently, as in BPE training.\n",
    "        \n",
    "        token_ids = []\n",
    "        # Purpose: Initialize list to store token IDs.\n",
    "        # Theory: Collects IDs for the final encoded sequence.\n",
    "        \n",
    "        for word in words:\n",
    "            # Purpose: Process each word in the input text.\n",
    "            # Theory: Encodes each word separately, handling spaces implicitly.\n",
    "            \n",
    "            tokens = list(word)\n",
    "            # Purpose: Split word into characters.\n",
    "            # Theory: Initializes tokenization at the character level.\n",
    "            \n",
    "            # Apply merge rules\n",
    "            for pair in self.merge_rules:\n",
    "                # Purpose: Iterate through merge rules in order.\n",
    "                # Theory: Applies merges in the order they were learned to ensure consistency.\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(tokens) - 1:\n",
    "                    # Purpose: Scan tokens for the current merge rule.\n",
    "                    # Theory: Checks for adjacent tokens matching the pair to merge.\n",
    "                    \n",
    "                    if (tokens[i], tokens[i + 1]) == pair:\n",
    "                        # Purpose: Check if current pair matches the merge rule.\n",
    "                        # Theory: Identifies mergeable tokens for replacement.\n",
    "                        \n",
    "                        tokens[i] = pair[0] + pair[1]\n",
    "                        # Purpose: Merge the pair into a new token.\n",
    "                        # Theory: Replaces two tokens with a single subword unit.\n",
    "                        \n",
    "                        tokens.pop(i + 1)\n",
    "                        # Purpose: Remove the second token of the pair.\n",
    "                        # Theory: Updates the token list after merging.\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        # Purpose: Move to the next position if no merge.\n",
    "                        # Theory: Continues scanning for mergeable pairs.\n",
    "            \n",
    "            # Convert tokens to IDs\n",
    "            for token in tokens:\n",
    "                # Purpose: Map each token to its ID.\n",
    "                # Theory: Uses vocabulary to convert subwords to IDs, falling back to character IDs for unknown tokens.\n",
    "                \n",
    "                if token in self.vocab:\n",
    "                    token_ids.append(self.vocab[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens\n",
    "                    for char in token:\n",
    "                        # Purpose: Split unknown token into characters.\n",
    "                        # Theory: Falls back to character-level encoding for robustness.\n",
    "                        \n",
    "                        token_ids.append(self.vocab.get(char, self.vocab.get('<unk>', -1)))\n",
    "                        # Purpose: Append character ID or -1 for unknown characters.\n",
    "                        # Theory: Ensures encoding continues even for unseen characters.\n",
    "        \n",
    "        return token_ids\n",
    "        # Purpose: Return the sequence of token IDs.\n",
    "        # Theory: Represents the tokenized text for LLM input.\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        # Purpose: Decode a sequence of token IDs back to text.\n",
    "        # Theory: Reconstructs text by mapping IDs to tokens and concatenating.\n",
    "        \n",
    "        tokens = [self.reverse_vocab.get(id, '<unk>') for id in token_ids]\n",
    "        # Purpose: Map token IDs to their string representations.\n",
    "        # Theory: Uses reverse vocabulary to convert IDs to subwords or characters.\n",
    "        \n",
    "        return ''.join(tokens)\n",
    "        # Purpose: Concatenate tokens to form the output text.\n",
    "        # Theory: Joins subwords without spaces, as BPE tokens are concatenated directly.\n",
    "\n",
    "# Test BPE implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Run a test of the BPE implementation.\n",
    "    # Theory: Demonstrates training, encoding, and decoding on a synthetic corpus.\n",
    "    \n",
    "    # Synthetic corpus\n",
    "    corpus = [\"hello world\", \"hello there\", \"world peace\", \"hi there\"]\n",
    "    # Purpose: Define a small text corpus for training.\n",
    "    # Theory: Mimics a dataset for LLMs, with repeated words to learn meaningful merges.\n",
    "    \n",
    "    bpe = BPE()\n",
    "    # Purpose: Initialize BPE model.\n",
    "    # Theory: Sets up vocabulary and merge rules for training.\n",
    "    \n",
    "    bpe.train(corpus, num_merges=100)\n",
    "    # Purpose: Train BPE on the corpus with 100 merges.\n",
    "    # Theory: Builds vocabulary and merge rules based on frequent pairs.\n",
    "    \n",
    "    print(f\"Vocabulary Size: {len(bpe.vocab)}\")\n",
    "    # Purpose: Print the size of the learned vocabulary.\n",
    "    # Theory: Expected to be ~113 (initial characters + 100 merges).\n",
    "    \n",
    "    print(f\"Sample Merge Rules: {bpe.merge_rules[:4]}\")\n",
    "    # Purpose: Print first few merge rules for inspection.\n",
    "    # Theory: Shows the most frequent pairs merged during training.\n",
    "    \n",
    "    # Test encoding\n",
    "    test_text = \"hello\"\n",
    "    encoded = bpe.encode(test_text)\n",
    "    # Purpose: Encode a test word.\n",
    "    # Theory: Converts text to token IDs using learned merge rules.\n",
    "    \n",
    "    print(f\"Encoded '{test_text}': {encoded}\")\n",
    "    # Purpose: Print encoded token IDs.\n",
    "    # Theory: Shows the tokenized representation of the input.\n",
    "    \n",
    "    decoded = bpe.decode(encoded)\n",
    "    # Purpose: Decode the token IDs back to text.\n",
    "    # Theory: Verifies that encoding is reversible.\n",
    "    \n",
    "    print(f\"Decoded {encoded}: {decoded}\")\n",
    "    # Purpose: Print decoded text.\n",
    "    # Theory: Should match the original input if encoding/decoding is lossless.\n",
    "    \n",
    "    # Test unseen word\n",
    "    test_text = \"hellothere\"\n",
    "    encoded = bpe.encode(test_text)\n",
    "    # Purpose: Encode an unseen word.\n",
    "    # Theory: Tests handling of out-of-vocabulary words using character fallback.\n",
    "    \n",
    "    print(f\"Encoded '{test_text}': {encoded}\")\n",
    "    # Purpose: Print encoded token IDs for unseen word.\n",
    "    # Theory: Shows robustness to new words.\n",
    "    \n",
    "    decoded = bpe.decode(encoded)\n",
    "    # Purpose: Decode the token IDs.\n",
    "    # Theory: Verifies correct reconstruction of the unseen word.\n",
    "    \n",
    "    print(f\"Decoded {encoded}: {decoded}\")\n",
    "    # Purpose: Print decoded text.\n",
    "    # Theory: Confirms lossless encoding/decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554854a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
