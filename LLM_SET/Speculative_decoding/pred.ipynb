{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bafe19",
   "metadata": {},
   "source": [
    "# Predictive Prefill with Speculative Decoding\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Title**: Implement Predictive Prefill with Speculative Decoding for LLMs\n",
    "\n",
    "**Description**: You are tasked with implementing Predictive Prefill with Speculative Decoding, a technique to accelerate inference in LLMs by predicting likely token sequences (prefill) and verifying them in parallel (speculative decoding). The system should use a small “draft” model to generate candidate sequences and a larger “target” model to verify them, reducing inference time. Implement a simplified version using PyTorch, with a draft model (small RNN) and target model (larger RNN), on a synthetic sequence dataset. The system should generate tokens, speculatively predict sequences, and verify them, measuring speedup.\n",
    "Mathematical Definition:\n",
    "\n",
    "Draft Model: Predicts the next token given a context:\n",
    "$$p_d(y_t | x_{1:t-1}; \\theta_d)$$\n",
    "where $ y_t $ is the next token, $ x_{1:t-1} $ is the context, $ \\theta_d $ are draft model parameters.\n",
    "Speculative Decoding:\n",
    "\n",
    "Generate $ k $ candidate tokens $ [y_t, y_{t+1}, \\ldots, y_{t+k-1}] $ from the draft model.\n",
    "Compute probabilities in parallel using the target model:\n",
    "$$p_t(y_i | x_{1:i-1}; \\theta_t), \\quad i = t, \\ldots, t+k-1$$\n",
    "\n",
    "Accept tokens where draft and target probabilities align within a threshold:\n",
    "$$|p_d(y_i) - p_t(y_i)| < \\epsilon$$\n",
    "\n",
    "Roll back to the last accepted token if verification fails.\n",
    "\n",
    "\n",
    "Loss:\n",
    "\n",
    "Train both models with cross-entropy loss:\n",
    "$$L = -\\sum_{t} \\log p(y_t | x_{1:t-1}; \\theta)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Implement a SpeculativeDecoder class with methods for:\n",
    "\n",
    "train: Train draft and target models on a sequence prediction task.\n",
    "decode: Perform speculative decoding, generating and verifying sequences.\n",
    "\n",
    "\n",
    "Use a synthetic dataset of integer sequences (e.g., arithmetic progressions).\n",
    "Draft model: Small RNN (1 layer, 32 hidden units).\n",
    "Target model: Larger RNN (2 layers, 64 hidden units).\n",
    "Speculative window: $ k = 3 $ tokens.\n",
    "Train for 100 epochs with cross-entropy loss and Adam optimizer (learning rate 0.001).\n",
    "Measure inference speedup compared to standard decoding.\n",
    "Provide detailed Purpose and Theory comments.\n",
    "\n",
    "Constraints:\n",
    "\n",
    "Use PyTorch for model implementation.\n",
    "No external libraries for decoding logic.\n",
    "Sequence length: 10 tokens, vocabulary size: 50 tokens.\n",
    "Handle batch processing for training.\n",
    "Use $ \\epsilon = 0.1 $ for verification threshold.\n",
    "\n",
    "Synthetic Dataset:\n",
    "\n",
    "Sequences: 100 sequences of 10 integers (e.g., [1, 2, 3, …, 10], [5, 10, 15, …, 50]).\n",
    "Vocabulary: Integers 0 to 49.\n",
    "Test Sequences: 3 sequences to test decoding.\n",
    "Task: Predict the next token in each sequence.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Training Loss: Decreases from ~3.0 to ~0.5 for both models.\n",
    "Decoding: Generates sequences with speculative decoding, accepting/rejecting tokens.\n",
    "Speedup: Speculative decoding is ~1.5–2x faster than standard decoding (measured in seconds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7410373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft Model Epoch [10/100], Loss: 3.8904\n",
      "Target Model Epoch [10/100], Loss: 3.8147\n",
      "Draft Model Epoch [20/100], Loss: 3.8239\n",
      "Target Model Epoch [20/100], Loss: 3.6881\n",
      "Draft Model Epoch [30/100], Loss: 3.7611\n",
      "Target Model Epoch [30/100], Loss: 3.5427\n",
      "Draft Model Epoch [40/100], Loss: 3.6991\n",
      "Target Model Epoch [40/100], Loss: 3.3781\n",
      "Draft Model Epoch [50/100], Loss: 3.6365\n",
      "Target Model Epoch [50/100], Loss: 3.1969\n",
      "Draft Model Epoch [60/100], Loss: 3.5727\n",
      "Target Model Epoch [60/100], Loss: 3.0011\n",
      "Draft Model Epoch [70/100], Loss: 3.5077\n",
      "Target Model Epoch [70/100], Loss: 2.7908\n",
      "Draft Model Epoch [80/100], Loss: 3.4415\n",
      "Target Model Epoch [80/100], Loss: 2.5677\n",
      "Draft Model Epoch [90/100], Loss: 3.3741\n",
      "Target Model Epoch [90/100], Loss: 2.3366\n",
      "Draft Model Epoch [100/100], Loss: 3.3058\n",
      "Target Model Epoch [100/100], Loss: 2.1028\n",
      "Standard Decoding Time: 0.0060s\n",
      "Speculative Decoding Time: 0.0080s\n",
      "Sequence: [1, 2, 3]\n",
      "Standard Decoding: [4, 40, 39, 8]\n",
      "Speculative Decoding: [9, 27, 41, 3, 38] (3/3 tokens accepted)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Purpose: Import PyTorch for tensor operations and model implementation.\n",
    "# Theory: PyTorch’s autograd supports training RNNs for sequence prediction.\n",
    "\n",
    "import torch.nn as nn\n",
    "# Purpose: Import neural network modules for RNN models.\n",
    "# Theory: nn.Module enables custom draft and target models.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Purpose: Import Adam optimizer for training.\n",
    "# Theory: Adam optimizes RNN parameters with adaptive learning rates.\n",
    "\n",
    "import time\n",
    "# Purpose: Import time for measuring inference speedup.\n",
    "# Theory: Tracks execution time to compare standard vs. speculative decoding.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "# Purpose: Fix random seed for consistent data and model initialization.\n",
    "# Theory: Aligns with previous problems for reproducibility.\n",
    "\n",
    "# Synthetic sequence dataset\n",
    "vocab_size, seq_len, num_sequences = 50, 10, 100\n",
    "# Purpose: Define dataset parameters: vocabulary size (50), sequence length (10), number of sequences (100).\n",
    "# Theory: Simulates token sequences for LLM training (e.g., tokenized text).\n",
    "\n",
    "sequences = torch.randint(0, vocab_size, (num_sequences, seq_len))\n",
    "# Purpose: Generate random integer sequences, shape [100, 10].\n",
    "# Theory: Mimics tokenized sequences, with integers as token IDs.\n",
    "\n",
    "# Define RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    # Purpose: Define RNN model for draft or target prediction.\n",
    "    # Theory: Processes sequences to predict next tokens.\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        # Purpose: Initialize RNN with specified parameters.\n",
    "        # Theory: Configures embedding, RNN, and output layers.\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        # Purpose: Call parent nn.Module constructor.\n",
    "        # Theory: Registers parameters for autograd.\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Purpose: Map token IDs to embeddings.\n",
    "        # Theory: Converts integers to dense vectors, shape [batch_size, seq_len, embed_dim].\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # Purpose: Define RNN layers.\n",
    "        # Theory: Processes sequences, outputting hidden states, shape [batch_size, seq_len, hidden_dim].\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        # Purpose: Define output layer for token prediction.\n",
    "        # Theory: Maps hidden states to token probabilities, shape [batch_size, seq_len, vocab_size].\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Purpose: Compute token probabilities and hidden states.\n",
    "        # Theory: Processes input sequence through embedding, RNN, and output layers.\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # Purpose: Convert token IDs to embeddings.\n",
    "        # Theory: Shape [batch_size, seq_len, embed_dim].\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # Purpose: Process embeddings through RNN.\n",
    "        # Theory: Outputs sequence representations and final hidden state.\n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        # Purpose: Map RNN outputs to token logits.\n",
    "        # Theory: Shape [batch_size, seq_len, vocab_size] for probability computation.\n",
    "        \n",
    "        return logits, hidden\n",
    "        # Purpose: Return logits and hidden state.\n",
    "        # Theory: Logits for loss computation, hidden for next step.\n",
    "\n",
    "# Define SpeculativeDecoder\n",
    "class SpeculativeDecoder:\n",
    "    # Purpose: Implement speculative decoding with draft and target models.\n",
    "    # Theory: Accelerates inference by predicting and verifying token sequences.\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=32, draft_hidden=32, target_hidden=64):\n",
    "        # Purpose: Initialize draft and target models.\n",
    "        # Theory: Draft model is smaller (1 layer, 32 units), target is larger (2 layers, 64 units).\n",
    "        \n",
    "        self.draft_model = RNNModel(vocab_size, embed_dim, draft_hidden, num_layers=1)\n",
    "        # Purpose: Initialize small draft model.\n",
    "        # Theory: Fast model for speculative predictions.\n",
    "        \n",
    "        self.target_model = RNNModel(vocab_size, embed_dim, target_hidden, num_layers=2)\n",
    "        # Purpose: Initialize larger target model.\n",
    "        # Theory: Accurate model for verifying predictions.\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        # Purpose: Store vocabulary size.\n",
    "        # Theory: Defines output space for token predictions.\n",
    "    \n",
    "    def train(self, sequences, epochs=100):\n",
    "        # Purpose: Train both draft and target models.\n",
    "        # Theory: Uses cross-entropy loss to learn sequence prediction.\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Purpose: Define cross-entropy loss.\n",
    "        # Theory: Measures prediction error for token sequences.\n",
    "        \n",
    "        draft_optimizer = optim.Adam(self.draft_model.parameters(), lr=0.001)\n",
    "        target_optimizer = optim.Adam(self.target_model.parameters(), lr=0.001)\n",
    "        # Purpose: Initialize Adam optimizers for both models.\n",
    "        # Theory: Adaptive learning rates for RNN training.\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Purpose: Iterate over epochs for training.\n",
    "            # Theory: Updates model parameters to minimize loss.\n",
    "            \n",
    "            self.draft_model.train()\n",
    "            self.target_model.train()\n",
    "            # Purpose: Set models to training mode.\n",
    "            # Theory: Enables gradient computation.\n",
    "            \n",
    "            draft_optimizer.zero_grad()\n",
    "            target_optimizer.zero_grad()\n",
    "            # Purpose: Reset gradients.\n",
    "            # Theory: Prevents gradient accumulation.\n",
    "            \n",
    "            # Prepare input and target (shifted by 1)\n",
    "            inputs = sequences[:, :-1]\n",
    "            targets = sequences[:, 1:]\n",
    "            # Purpose: Create input-target pairs for next-token prediction.\n",
    "            # Theory: Inputs are [t_1, ..., t_{n-1}], targets are [t_2, ..., t_n].\n",
    "            \n",
    "            # Draft model\n",
    "            draft_logits, _ = self.draft_model(inputs)\n",
    "            # Purpose: Compute draft model predictions.\n",
    "            # Theory: Shape [100, 9, vocab_size] for loss computation.\n",
    "            \n",
    "            draft_loss = criterion(draft_logits.reshape(-1, self.vocab_size), targets.reshape(-1))\n",
    "            # Purpose: Compute draft model loss.\n",
    "            # Theory: Cross-entropy loss over all tokens.\n",
    "            \n",
    "            draft_loss.backward()\n",
    "            draft_optimizer.step()\n",
    "            # Purpose: Update draft model parameters.\n",
    "            # Theory: Applies gradient-based updates.\n",
    "            \n",
    "            # Target model\n",
    "            target_logits, _ = self.target_model(inputs)\n",
    "            # Purpose: Compute target model predictions.\n",
    "            # Theory: Shape [100, 9, vocab_size] for loss computation.\n",
    "            \n",
    "            target_loss = criterion(target_logits.reshape(-1, self.vocab_size), targets.reshape(-1))\n",
    "            # Purpose: Compute target model loss.\n",
    "            # Theory: Cross-entropy loss for accurate model.\n",
    "            \n",
    "            target_loss.backward()\n",
    "            target_optimizer.step()\n",
    "            # Purpose: Update target model parameters.\n",
    "            # Theory: Applies gradient-based updates.\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # Purpose: Print loss every 10 epochs.\n",
    "                # Theory: Monitors training progress.\n",
    "                \n",
    "                print(f\"Draft Model Epoch [{epoch + 1}/{epochs}], Loss: {draft_loss.item():.4f}\")\n",
    "                print(f\"Target Model Epoch [{epoch + 1}/{epochs}], Loss: {target_loss.item():.4f}\")\n",
    "    \n",
    "    def standard_decode(self, context, max_len=4):\n",
    "        # Purpose: Perform standard decoding with the target model.\n",
    "        # Theory: Generates one token at a time, baseline for speedup comparison.\n",
    "        \n",
    "        self.target_model.eval()\n",
    "        # Purpose: Set target model to evaluation mode.\n",
    "        # Theory: Disables gradient computation for inference.\n",
    "        \n",
    "        generated = context.tolist()\n",
    "        # Purpose: Convert context tensor to list.\n",
    "        # Theory: Initial sequence to start generation.\n",
    "        \n",
    "        hidden = None\n",
    "        # Purpose: Initialize hidden state for RNN.\n",
    "        # Theory: Tracks sequence context.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Purpose: Disable gradient tracking.\n",
    "            # Theory: Saves memory during inference.\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                # Purpose: Generate max_len tokens.\n",
    "                # Theory: Extends sequence one token at a time.\n",
    "                \n",
    "                input_tensor = torch.tensor([generated], dtype=torch.long)\n",
    "                # Purpose: Convert current sequence to tensor.\n",
    "                # Theory: Shape [1, len(generated)] for RNN input.\n",
    "                \n",
    "                logits, hidden = self.target_model(input_tensor, hidden)\n",
    "                # Purpose: Compute next token probabilities.\n",
    "                # Theory: Uses last hidden state for continuity.\n",
    "                \n",
    "                probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "                # Purpose: Convert logits to probabilities.\n",
    "                # Theory: Shape [1, vocab_size] for token selection.\n",
    "                \n",
    "                next_token = torch.argmax(probs, dim=-1).item()\n",
    "                # Purpose: Select most likely token.\n",
    "                # Theory: Greedy decoding for simplicity.\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                # Purpose: Append token to sequence.\n",
    "                # Theory: Builds the generated sequence.\n",
    "        \n",
    "        return generated[len(context):]\n",
    "        # Purpose: Return generated tokens (excluding context).\n",
    "        # Theory: Outputs continuation of the input sequence.\n",
    "    \n",
    "    def speculative_decode(self, context, max_len=4, k=3, epsilon=0.1):\n",
    "        # Purpose: Perform speculative decoding with draft and target models.\n",
    "        # Theory: Predicts k tokens with draft model, verifies with target model.\n",
    "        \n",
    "        self.draft_model.eval()\n",
    "        self.target_model.eval()\n",
    "        # Purpose: Set models to evaluation mode.\n",
    "        # Theory: Disables gradients for inference.\n",
    "        \n",
    "        generated = context.tolist()\n",
    "        # Purpose: Convert context tensor to list.\n",
    "        # Theory: Initial sequence for generation.\n",
    "        \n",
    "        draft_hidden = None\n",
    "        target_hidden = None\n",
    "        # Purpose: Initialize hidden states for both models.\n",
    "        # Theory: Tracks sequence context for RNNs.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Purpose: Disable gradient tracking.\n",
    "            # Theory: Saves memory during inference.\n",
    "            \n",
    "            while len(generated) - len(context) < max_len:\n",
    "                # Purpose: Generate until max_len tokens are added.\n",
    "                # Theory: Extends sequence with speculative predictions.\n",
    "                \n",
    "                # Draft model predicts k tokens\n",
    "                draft_tokens = []\n",
    "                temp_generated = generated.copy()\n",
    "                temp_draft_hidden = draft_hidden\n",
    "                # Purpose: Initialize temporary sequence and hidden state.\n",
    "                # Theory: Allows rollback if verification fails.\n",
    "                \n",
    "                for _ in range(k):\n",
    "                    # Purpose: Predict k tokens with draft model.\n",
    "                    # Theory: Generates speculative sequence.\n",
    "                    \n",
    "                    input_tensor = torch.tensor([temp_generated], dtype=torch.long)\n",
    "                    # Purpose: Convert current sequence to tensor.\n",
    "                    # Theory: Shape [1, len(temp_generated)] for draft model.\n",
    "                    \n",
    "                    logits, temp_draft_hidden = self.draft_model(input_tensor, temp_draft_hidden)\n",
    "                    # Purpose: Compute draft model predictions.\n",
    "                    # Theory: Outputs logits and updates hidden state.\n",
    "                    \n",
    "                    probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "                    # Purpose: Convert logits to probabilities.\n",
    "                    # Theory: Shape [1, vocab_size] for token selection.\n",
    "                    \n",
    "                    next_token = torch.argmax(probs, dim=-1).item()\n",
    "                    # Purpose: Select most likely token.\n",
    "                    # Theory: Greedy decoding for draft predictions.\n",
    "                    \n",
    "                    draft_tokens.append(next_token)\n",
    "                    temp_generated.append(next_token)\n",
    "                    # Purpose: Append token to temporary sequence.\n",
    "                    # Theory: Builds speculative sequence.\n",
    "                \n",
    "                # Verify with target model\n",
    "                input_tensor = torch.tensor([generated + draft_tokens], dtype=torch.long)\n",
    "                # Purpose: Prepare input with draft tokens.\n",
    "                # Theory: Shape [1, len(generated) + k] for target model verification.\n",
    "                \n",
    "                target_logits, target_hidden = self.target_model(input_tensor, target_hidden)\n",
    "                # Purpose: Compute target model probabilities.\n",
    "                # Theory: Verifies all k tokens in one pass.\n",
    "                \n",
    "                accepted = 0\n",
    "                # Purpose: Track number of accepted tokens.\n",
    "                # Theory: Counts tokens where draft and target agree.\n",
    "                \n",
    "                for i in range(len(draft_tokens)):\n",
    "                    # Purpose: Verify each draft token.\n",
    "                    # Theory: Compares draft and target probabilities.\n",
    "                    \n",
    "                    draft_prob = torch.softmax(self.draft_model(torch.tensor([generated + draft_tokens[:i+1]]))[0][:, -1, :], dim=-1)\n",
    "                    target_prob = torch.softmax(target_logits[:, len(generated) + i - len(context), :], dim=-1)\n",
    "                    # Purpose: Compute probabilities for the current token.\n",
    "                    # Theory: Shape [1, vocab_size] for comparison.\n",
    "                    \n",
    "                    if abs(draft_prob[0, draft_tokens[i]] - target_prob[0, draft_tokens[i]]) < epsilon:\n",
    "                        # Purpose: Check if probabilities are within threshold.\n",
    "                        # Theory: Accepts token if draft and target agree.\n",
    "                        \n",
    "                        accepted += 1\n",
    "                        generated.append(draft_tokens[i])\n",
    "                        # Purpose: Append accepted token.\n",
    "                        # Theory: Extends sequence with verified token.\n",
    "                    else:\n",
    "                        break\n",
    "                        # Purpose: Stop at first rejected token.\n",
    "                        # Theory: Rolls back to last accepted token.\n",
    "                \n",
    "                draft_hidden = temp_draft_hidden if accepted == k else None\n",
    "                # Purpose: Update draft hidden state if all tokens accepted.\n",
    "                # Theory: Maintains continuity for next iteration.\n",
    "                \n",
    "                if accepted == 0:\n",
    "                    # Purpose: Handle case where no tokens are accepted.\n",
    "                    # Theory: Falls back to standard decoding for one token.\n",
    "                    \n",
    "                    input_tensor = torch.tensor([generated], dtype=torch.long)\n",
    "                    logits, target_hidden = self.target_model(input_tensor, target_hidden)\n",
    "                    probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "                    next_token = torch.argmax(probs, dim=-1).item()\n",
    "                    generated.append(next_token)\n",
    "                    draft_hidden = None\n",
    "                    # Purpose: Append one verified token.\n",
    "                    # Theory: Ensures progress even if speculation fails.\n",
    "        \n",
    "        return generated[len(context):], accepted\n",
    "        # Purpose: Return generated tokens and number accepted.\n",
    "        # Theory: Outputs continuation and speculation success rate.\n",
    "\n",
    "# Test SpeculativeDecoder\n",
    "if __name__ == \"__main__\":\n",
    "    # Purpose: Test speculative decoding implementation.\n",
    "    # Theory: Trains models and compares standard vs. speculative decoding.\n",
    "    \n",
    "    decoder = SpeculativeDecoder(vocab_size)\n",
    "    # Purpose: Initialize decoder with draft and target models.\n",
    "    # Theory: Sets up models for sequence prediction.\n",
    "    \n",
    "    decoder.train(sequences, epochs=100)\n",
    "    # Purpose: Train both models on the dataset.\n",
    "    # Theory: Optimizes for next-token prediction.\n",
    "    \n",
    "    context = torch.tensor([1, 2, 3])\n",
    "    # Purpose: Define test context for decoding.\n",
    "    # Theory: Simulates initial sequence for generation.\n",
    "    \n",
    "    # Standard decoding\n",
    "    start_time = time.time()\n",
    "    standard_output = decoder.standard_decode(context)\n",
    "    standard_time = time.time() - start_time\n",
    "    # Purpose: Perform and time standard decoding.\n",
    "    # Theory: Baseline for speed comparison.\n",
    "    \n",
    "    # Speculative decoding\n",
    "    start_time = time.time()\n",
    "    speculative_output, accepted = decoder.speculative_decode(context, k=3)\n",
    "    speculative_time = time.time() - start_time\n",
    "    # Purpose: Perform and time speculative decoding.\n",
    "    # Theory: Tests speedup and accuracy of speculation.\n",
    "    \n",
    "    print(f\"Standard Decoding Time: {standard_time:.4f}s\")\n",
    "    print(f\"Speculative Decoding Time: {speculative_time:.4f}s\")\n",
    "    print(f\"Sequence: {context.tolist()}\")\n",
    "    print(f\"Standard Decoding: {standard_output}\")\n",
    "    print(f\"Speculative Decoding: {speculative_output} ({accepted}/3 tokens accepted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dc69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
